\title{Inductive Transformers: Inductive Bias Is What We Need}


\section{Abstract}

\section{Introduction}

\section{Prior Art}

\section{The Inductive Transformer Model}



\section{Summary Comparison of Original Transformer and Inductive Transformer}

\section{A Reminder of Original Self-Attention}



\section{Inference in the Inductive Transformer}

\section{Representing the Data}

\subsection{From Indicator Vectors to Embedding Vectors}

\section{README: Decoder Marginalization}

\subsection{Simplifying Coins-to-Die and Die-to-Coins}

\subsection{Decoder Inference}

\section{Weights}

\subsection{Encoder Inference}

\section{Layers}

\subsection{Original Linear Activation Function $\rightarrow$ Computing the Posterior Log Probability of a Bayesian AND}

\subsection{Original ReLu-based Feed-Forward Layer $\rightarrow$ Computing the Posterior Log Probability for a Categorical Distribution}

\section{Network Connectivity}

\subsection{Original Residual Connections $\rightarrow$ Strong Lexicalization in the Inductive Transformer}

\subsection{Connecting the Encoder to the Decoder}

\section{Results}

\subsection{Concept Organization}

\subsection{Concept Nucleation}

\subsection{Identifiability}

\section{Discussion}

\section{Ethics}

% ---- new_verbiage_unexploited

\section{What is Inductive Bias} % ---- new_verbiage_unexploited

\section{notes about our code} % ---- new_verbiage_unexploited

\section{Linear versus Nonlinear} % ---- new_verbiage_unexploited

\section{on log probabilities versus probabilities} % ---- new_verbiage_unexploited


\section{bias and abstraction} % ---- new_verbiage_unexploited

\subsection{Symbolic methods and neural networks} % ---- new_verbiage_unexploited

\subsection{can't have models of physics built into it nor do arithmetic} % ---- new_verbiage_unexploited

\subsection{just predicts the next word, has no abstract conceptual understanding} % ---- new_verbiage_unexploited


\subsection{can't assess it's own certainty accurately} % ---- new_verbiage_unexploited

\subsection{just a reflection of human thought from the text it used for training} % ---- new_verbiage_unexploited

\subsection{can't extrapolate, can only interpolate} % ---- new_verbiage_unexploited

\subsection{do not self reflect, playing conterfactual in their mind like we do} % ---- new_verbiage_unexploited

\subsection{phase transitions/emergent behavior in AI} % ---- new_verbiage_unexploited

\subsection{sparse coding embeddings are central to the approach} % ---- new_verbiage_unexploited


\section{to_decoder} % ---- new_verbiage_unexploited


\section{Representing an open-universe in a closed universe model} % ---- new_verbiage_unexploited


\section{Encoder Exactly-One: Independent Variables} % ---- new_verbiage_unexploited


\section{Encoder Exactly-One: Joint Variable} % ---- new_verbiage_unexploited

\subsection{Decoder Categorical} % ---- new_verbiage_unexploited

\subsection{Decoder All} % ---- new_verbiage_unexploited


% APPENDIX

\section{Enabling the Incorporation of Generative (Bayesian) Scientific Models Into Large Language Models}

\section{From Indicator Vectors to Embedding Vectors} % ---- new_verbiage_unexploited