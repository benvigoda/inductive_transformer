(b.) $\pi_P$ involves categorical choices over relative positions in the data window in order to implement the attention mechanism, explaining away data that is comprised of a token at each position in the (sliding) data window. (c.) To aid with discussion of how we represent an open-universe probabilistic program in a closed universe deep learning network, we will use a rounded rectangle to represent just the right arm of the production that chooses from productions in the layer below (and temporarily omit the left arm which chooses over tokens and token positions).

------

Translate from text to text tagged with level 1 abstractions, level 2 abstractions, and so forth







\section{Ethics.} Currently, SOA large language models have a trillion parameters and they are growing.  The carbon emissions from training them are significant.  inductive transformers models train to same level of performance faster, and require less data for training.  This can save carbon emissions.


SOA large language models cost millions of dollars per experimental training run.  Iterating for model architecture exploration is expensive.  Only a few large organizations can afford to do so, and within those organizations, sub-organizations compete for resources.  inductive transformers models train to the same level of performance with far less computing resources which means that a wide variety of smaller organizations and even individuals will be able to train inductive transformers models as well as improve upon them.

SOA language models require x TFLOPS of compute to train, currently requiring weeks to train on very large server farms.  The compute expense bounds the rate at which new architectures can be explored.  inductive transformers models can train to the same level of performance with orders of magnitude fewer compute resources, enabling faster model exploration.

Large language models are black-box, so it is not easy to look into them to determine what improvements can or should be made.  In inductive transformers models, it is very clear what is happening inside.  This could accelerate the science of developing new models.


In summary, inductive transformers models enable rapid model exploration by individuals or smaller organizations with much more readily available compute resources. It is also more clear what kinds of changes could or should be made in order to further improve them.  A new idea could be explored in minutes or hours instead of days, weeks or months…

The result could be faster model improvements by a larger number of smaller research groups.

SOA large language models are “black boxes”.  We cannot inspect into them in order to see what their prediction would have been if a latent assumption was changed.  With inductive transformers models we can make these kinds of counterfactual inspections.


There is also nothing in principle that would prevent connecting the model to itself in order to allow the model to introspect itself, and to perform its own counterfactual reasoning involving its own beliefs or architecture.

We do not know what the consequences will be of a new class of rapidly evolving SOA models that can perform counterfactual introspection on themselves and retrain or rewrite themselves very quickly. 




This article has no additional data.

\textbf{Authors’ contributions.} B.V.:  conceptualization, supervision, methodology, writing—original draft, writing—review and editing, pair-coding, running simulations. T.R.:  writing—review and editing, pair-coding, running simulations.  Both authors gave final approval for publication and agreed to be held accountable for the work performed therein.

\textbf{Conflict of interest declaration.} We declare we have no competing interests.


\section{bayes factor comparison of architectures}

Since we are able to exactly (or nearly exactly) marginalize the inductive transformer to calculate posterior probabilities, if we had two different inductive transformers, we can determine which one has a higher posterior probability when fitting the same natural language corpus.  For example, we could put both (trained) models under an $\pi$ and learn the weights on each branch of this choice over models.  This is called a Bayes factor probabilistic program, and it would provide a sort of a/b test of the two models.  Since each model would also contain $\pi$ over concepts in its hierarchy, we see that these models are already performing a kind of hypothesis testing at many different levels of abstraction.


\subsection{Enabling the Incorporation of Generative (Bayesian) Scientific Models Into Large Language Models}

Tenenbaum et. al. suggest incorporating probabilistic programming languages into the workflows of large language models, so that in the future they can reason more precisely about scientific problems.  Human scientists use tools such as probabilistic programs to numerically evaluate how well theories fit experimental data.  These authors suggest that AI scientists could do the same thing by being trained to write probabilistic programs.

This would mean, however that the generative explanation of the data would be a theory expressed in language that is then translated to probabilistic program code and which then generates the data.

In this situation, there is only one probabilistic program being considered by the model at any one time.  If there are multiple probabilistic programs to consider as hypotheses, just like a human scientist, the language model has to write each probabilistic program.

Now that we have expressed a language model as an engineerable Bayesian model, and shown how to express open-universe models in a closed-universe network, we can do something rather different.  We can incorporate the building blocks of probabilistic programs directly into the large language model and perform \emph{end-to-end learning}.  Across the literature, end-to-end learning has been shown to out-perform independent models that are pipe-lined together~\cite{end to end learning outperforms pipelined models}. 

The analogy in human intelligence is John von Neuman who not only could calculate mathematical models in his head at lightning speeds, he could also think incredibly creatively.  This made him much more capable than other humans who were equally creative, but who had to rely on external electronic computers in order to perform their detailed modeling and calculations.\cite{} John von Neumann had access to the same data as his peers.  What made him capable of 

\section{Ethics}


\section{notes about our code}

Set up of our identifiability experiment code

In practice our code does not implement a decoder-only forward model that samples traces from the root and generates data according to the decoder-only weights.

We actually manually set the weights in an encoder-decoder forward model, prompt it, and have it generate synthetic data sentences.

This means that we would not expect an exact match between the activations in an encoder-only inverse model and the encoder of the encoder-decoder forward model.  We also would not expect the activations in the decoder of an encoder-decoder inverse model to match any specific activations in the forward model.

If we generated from a decoder-only forward model with specific weights, would we expect a encoder-decoder inverse model to match its weights?  No, right?

What would we expect to match perfectly?  A forward model and an inverse model that are both the same, and therefore both encoder-decoder.  And we need their prompts to be the same as well.

For training identifiability, if we set the weights manually in the forward model, prompt the forward model with inputs that do not tilt it in any particular direction, then it will generate training data solely according to its weights.  If we train the inverse model on this data, then the learned weights in the inverse model will match (modulo some (left-right, top-bottom) symmetries the weights of the forward model.  If the weights in the forward model are not symmetric then the inverse model should learn to put the weights in the same places as the forward model.

\section{Does The Inductive Transformer Contain Sufficient Nonlinearity To Be Interesting?}

The vanilla transformer, comprised of sums and relu's is highly nonlinear. 
 Marginalization of the inductive transformer model (in the probability domain) is entirely comprised of sums of products.  Is the inductive transformer sufficiently nonlinear to be interesting?

Naively, the computational complexity of marginalization of any joint distribution is an exponential in the number of variables to be marginalized out.\cite{}

Any nonlinear function in a smaller state space can be embedded within a linear function in a larger state space.\cite{universal set of Tofolli gates}\cite{universal set of Fredkin gates}



\section{on log probabilities versus probabilities}

on the topic of whether we should multiply by betas versus exponentiate by them, it may just a matter of numerical precision the first layer of neurons in a network, if they approximate sigmoid can effectively take a log of input probabilities or effectively keep them as probabilities
if they are weighted ReLu's, they can effectively take a log of input probabilities

then if we exponentiate such a value by a beta or multiply it by a beta, gradient descent can choose values for the beta and the transformation of the input that to create a similar output whether we are treating activations as probabilities or as log probabilities

interestingly, this same thing came up in my analog chips.  we had an analog circuit approximating a soft-XOR and a way that I saved the company was to realize that it could approximate the soft-XOR to second order (which was all we needed) no matter whether the input currents were considered to be log-probabilities or probabilities.  We should look back at that.

---

PAPER ABOUT TEMPERATURE/BETAS AS WEIGHTS:
- investigate the good news of why it is making temperatures that are good for us
- also good news that it is not wandering onto the flat plains of oblivion
- plot at the energy surface for X1LSE and X1LSE_decode where loss is a function of input log probabilities and where 
loss is a function of the weights on the inputs
- There is something to be said about the fact that this model is actually learning the ideal temperatures compared to current
GPT models where the user has to somewhat arbitrarily guess the "right" temperature for the task they are trying to accomplish

---

do activations in a model like LLAMA or FALCON look like they are distributed as if they are log probabilities
namely, there is a max (0), but could be constant offset, so maybe it is +217 or something with 
a long tail of activations spread out towards -infinity?  or are they nornally distributed around some mean,
more like probabilities? 

say they are distributed like log probabilities, why is the world/math/brains multiplying them by weights?  it's not really a temperature because there is no normalization in the probability domain after the application
of the temperature, and it's also definitely not a categorical weight

say they are normally distributed like probabilities, then it makes sense that we are multiplying them by weights
and you could forgive the lack of normalization in a neural network as stemming from the fact that a priori we
biological evolution didn't know which variables to consider jointly, so it doesn't know what groups of variables to 
normalize over before training begins.  what we are doing with inductive bias is basically putting some prior structure in
in terms of where we want jointness and which variables should be grouped together as mutually exclusive.

\section{bias and abstraction}

we want improbable*strong == near zero probability.
we want probable*weak  == near zero probability.
do we also want improbable*weak == probable*weak

if it never sees "the cat" nor "a dog" and other combinations that are not present in the training set, and meanwhile the trainign set contains 300 samples each of "the dog" and "a cat", then it should conclude that the relative ratio, strong*probable / weak*improbable > 300

greater abstraction would help get these ratios more quickly because it would concentrate statistical strength

animals are likely. under animals there are real safari animals and also mythical beasts
cats dogs unicorns dragons
because animals are likely in general, we might have to work harder to believe that mythical beasts are improbable


\subsection{Symbolic methods and neural networks}

**** USE THIS ***

When temperature is very low and/or weights and activations are very extreme, the inductive transformer becomes a logical grammar of the following form

\subsection{can't have models of physics built into it nor do arithmetic}

show how it is a grammatical multi-tree and that generative models of physics or other scientific models could be incorporated as to explain away 

show how you could bake arithmetic in as part of the model to explain away the input, and predict the output (would need this section to be very low temperature)

\subsection{just predicts the next word, has no abstract conceptual understanding}

show abstract concept tagging benchmark

\subsection{can't assess it's own certainty accurately}

\subsection{just a reflection of human thought from the text it used for training}

could be making it editable
and also able to edit itself

\subsection{can't extrapolate, can only interpolate}

this is not true.  it can combine existing concepts in a novel way into a new higher level concept.  show graphical demonstration.

learning in high dimensions is always extrapolation:
https://arxiv.org/abs/2110.09485

\subsection{do not self reflect, playing conterfactual in their mind like we do}

show how to run multiple generations to get different high probability interpretations of the data

show how to 

\subsection{phase transitions/emergent behavior in AI}

what humans observe as phase transitions or new emergent behaviors in AI come from gaining a next layer of abstraction

\subsection{sparse coding embeddings are central to the approach}

perhaps.  talk a little bit about the projection operator for this




\section{to_decoder}

the decoder ALLSUM has three incident edges, $x_{\mathrm{encode}}$, $y_{\mathrm{encode}}$, and $z_{\mathrm{decode}}$.  When computing the outgoing message $y_{\mathrm{decode}}$, it requires input messages from both $x_{\mathrm{encode}}$ and $z_{\mathrm{decode}}$.  Similarly, when computing the outgoing message $x_{\mathrm{decode}}$, the decoder ALLSUM requires input messages from both $y_{\mathrm{encode}}$ and $z_{\mathrm{decode}}$.

We send $x_{\mathrm{encode}}$ and $y_{\mathrm{encode}}$ from the encoder to the decoder to make the necessary information available to the decoder ALLSUM nodes.

In the conventional transformer, the information contained in $x_{\mathrm{encode}}$ is available to the decoder from the residual (ie. skip) connections which convey the prompt input into the decoder.  The decoder needs to learn to extract the $x_{\mathrm{encode}}$ information it needs from the raw data input.  The information in $y_{\mathrm{encode}}$ is less directly available to the decoder in a conventional transformer. The decoder must learn to convey this information from the encoder to the decoder indirectly by learning to thread it through all of the layers of the encoder.  When training begins and we randomly initialize the parameters of the model, before this threading has been learned, the decoder is operating without the benefit of access to $y_{\mathrm{encode}}$.  That means that the decoder computes its output $x_{\mathrm{decode}}$, a distribution over words without knowing which concepts the input has activated in the encoder.  It is possible that the lack of these connections may slow the speed of learning convergence in the conventional transformer.




\section{Encoder Exactly-One: Independent Variables}

We start with a set of random variables, $w_i$, each one representing the participation of a single word or concept in the generation from the model.  So for example, the Bernoulli random variable (a weighted coin) $w_{dog}$ could be 90\% likely to generate the word `dog' and 10\% likely to not generate the word `dog'.

A group of Bernoulli variables can be grouped together.  For example, we could have, $w_{dog}$, $w_{cat}$, $w_{collar}$, $w_{toy}$ each providing a probability for a particular word to be generated.

We have another random variable, X. Think of X as another coin that we flip in order to decide whether or not we will flip any of the word coins.

We can write a joint probability distribution over all of these coins,

\begin{equation}
    p(X, w_dog, w_cat, w_collar, w_toy). \\
\end{equation}




We factor the joint distribution using Bayes' rule and then marginalize to obtain the marginal probability of the existence Bernoulli.

GOOD PLOT FOR EXTENDED LENGTH PAPER:
- Is X1LSE layer like a winner takes all layer?  - if we input to X1LSE (and X1LSE_decode) a word_log_prob_tensor like (-0.5, -1.5) 
then what does it output?  we would expect an output like (-0.4, -1.7) without any weights in the picture?


\section{Encoder Exactly-One: Joint Variable}

We start with a single multistate random variable, W, where each state of this variable represents the participation of a particular word or concept.  For example, the states of W could be $\in "dog", "cat", "food", "toy"$.


We have two ways that we could represent a distribution over words: (1) a Bernoulli distribution for each word in the vocabulary, or (2) a Categorical distribution over all words.

In both cases, 

When each word probability is represented by a Bernoulli random variable, the p(x) message is computed as,

\begin{equation}
    p(x) = \sum_{word_1 \in 0,1}\ldots \sum_{word_N \in 0,1} p(x|word_1, \ldots, word_N)p(word_1)\ldots p(word_N).  \\
\end{equation}
The complexity of this computation is $O(2^vocab_size)$.


By contrast, when word probabilities are represented by a Categorical distribution over words, then the p(x) message is computed as,

\begin{equation}
    p(x) = \sum_{word \in words} p(x|word)p(word) \\
\end{equation}

\begin{equation}
    p(x=1) = \sum_{word \in words} weight_{word} activation_{word} \\
\end{equation}

This is an inner product of incoming activations with their corresponding weights.  When the incoming activations represent words in an indicator-function embedding, this has complexity is $O(vocab_size)$.  When the incoming activations represent concepts, the complexity is $O(layer_width)$.  

In either case, we need one of these computations for each ``categorical'' neuron in a layer.

\subsection{Decoder Categorical}

In the decoder, we compute p(word) from an incoming distribution over existence.

\begin{eqnarray}
    p(word_1) = \sum_{x \in 0,1} p(word_1 | x)p(x), \\
    p(word_2) = \sum_{x \in 0,1} p(word_2 | x)p(x), \\
    \ldots \\
    p(word_N) = \sum_{x \in 0,1} p(word_N | x)p(x), \\
\end{eqnarray}

By contrast, when word probabilities are represented by a Categorical distribution over words, then we compute

\begin{equation}
    p(word) = \sum_{x \in 0, 1} p(word|x)p(x) \\
\end{equation}

This is an inner product of copies of the incoming activation $p(x)$ with the corresponding weights.

\subsection{Decoder All}

Bernoulli based approach

\begin{equation}
    p(z) = \sum_{x \in 0, 1}\sum_{y \in 0, 1} p(x, y, z) \\
\end{equation}

\begin{equation}
    p(z) = \sum_{x \in 0, 1}\sum_{y \in 0, 1} p(z|x, y)p(x)p(y) \\
\end{equation}

\begin{equation}
    p(z) = \sum_{x \in 0, 1}\sum_{y \in 0, 1} p(z|x, y)p(x)p(y) \\
\end{equation}

Categorical based approach

z a  probability
0 00 1
0 01 1
0 10 1
0 11 0
1 00 0
1 01 0
1 10 0
1 11 1

\begin{equation}
    p(z) = \sum_{a \in x, y} p(z, a) \\
\end{equation}

where $a \in \{x, y\}$.

\begin{equation}
    p(z) = \sum_{a \in xy = 00, 01, 10, 11} p(z|a) p(a) \\
\end{equation}

\begin{equation}
    p(z=1) = \sum_{a \in xy = 00, 01, 10, 11} p(z=1|a) p(a) \\
\end{equation}

\begin{equation}
    p(z=1) = p(z=1|a=11) p(a=11) \\
    p(z=0) = p(z=0|a=01) p(a=01) + p(z=0|a=10) p(a=10) + p(z=0|a=00) p(a=00) \\
\end{equation}

\begin{equation}
    p(z=1) = p(a=11) \\
    p(z=0) = p(a=01) + p(a=10) + p(a=00) \\
\end{equation}

need a converter from $a=xy$ to $p(x)$ and $p(y)$


\section{temperature}

the whole idea about how when beta is inverse
temperature, a $\beta = 1$ maintains a signal
while a $\beta= 1e-9$ washes out a signal

\section{John von Neumann}

By the time von Neumann ``was six years old, he could divide two eight-digit numbers in his head and could converse in Ancient Greek. ... By the age of eight, von Neumann was familiar with differential and integral calculus, and by twelve he had read and understood Borel's Théorie des Fonctions.''

Nobel Laureate Hans Bethe said "I have sometimes wondered whether a brain like von Neumann's does not indicate a species superior to that of man",[40] and later Bethe wrote that "[von Neumann's] brain indicated a new species, an evolution beyond man".[455] Paul Halmos states that "von Neumann's speed was awe-inspiring."[420] Israel Halperin said: "Keeping up with him was ... impossible. The feeling was you were on a tricycle chasing a racing car."[456] Edward Teller admitted that he "never could keep up with him".[457] Teller also said "von Neumann would carry on a conversation with my 3-year-old son, and the two of them would talk as equals, and I sometimes wondered if he used the same principle when he talked to the rest of us."[458] Peter Lax wrote "Von Neumann was addicted to thinking, and in particular to thinking about mathematics".[453] Claude Shannon called him "the smartest person I've ever met", a common opinion.[459]

von Neumann had access to the same training data as his peers.  He did not have access to more training data; The bandwidth of the human ears and eyes is tightly bounded.  The reality is that data is not the sole contribution to the development of a mind.  The reality is that some minds are simply different than others.  They have a different inductive bias.


\section{ICLR topics}


unsupervised, self-supervised, semi-supervised, and supervised representation learning
representation learning for planning, reinforcement learning, and lifelong learning
representation learning for computer vision, audio, language, and other modalities
metric learning and kernel learning
sparse coding and dimensionality expansion
hierarchical models
uncertainty quantification
Bayesian and probabilistic methods, optimal transport
optimization
theoretical issues in deep learning
societal considerations including fairness, safety, privacy, and interpretability
visualization or interpretation of learned representations
implementation issues, parallelization, software platforms, hardware
applications in speech, robotics & autonomy, programming, formal reasoning, neuroscience & cognitive science, chemistry, biology, physics, or any other field


\section{Interpretability}

Inductive bias may or may not be correlated with interpretability.  Adding inductive bias could result in an equally black box model.

In practice, however, to come up with useful inductive bias we generally need to have some idea about what the shape of good theories of our data might look like.  In our case, our idea comes from noticing that probabilistic multi-grammars live within a conventional transformer architecture and could constitute a kind of ``language'' instinct for transformers.

\section{Connection to Hidden Markov Models}

% If layer_width = 1 and num_layers = 5, then our model is equivalent to a Hidden Markov Model with binary variables everywhere (both hidden and observed), and with independently learned weights for every transition matrix and every observation matrix.

% Is this even identifiable?  Maybe you could if it is short and you give it more than one short data sequence to learn on?  That seems likely to converge.

% - if one has an HMM, the entire literature is about having a finite but long sequence (10k's of data points)
% a single transition matrix T and a single observation matrix O that have to work across all transitions and observations.
% (not a ton of variables for the amount of the data )

% HMM:
% h_1 -> T_1 matrix -> h_2 
% ^
% O_1 matrix
% ^
% o_1

% - think about our situation.  An HMM where you can have each transition matrix, T_1, T_2, ... T_N contain different weights
% and similarly for the observation matrices O_1, ...  and very small data so far, just 1... N.

% in this situation, we could have o_1 = (a,b,c) = (1,0,0) (which means 100% probability of having observed 'a' at the first time step), 
% And then o_2 = (0,1,0), then o_3 = (0,0,1), and then o_4 = (1,0,0)

% If O_1 = O_2 = O_3 = 
% (1/3, 1/3, 1/3; 
% 1/3, 1/3, 1/3; 
% 1/3, 1/3, 1/3), 

% remembering that weights must be normalized - we should check if we impose this constraint on our word_X1LSE.  

% (Note: We have temperatures, not stochastic matrices in the same sense as this, so we should understand this.
% Temperatures could be infinite which would wash out any distribution so that later layers of the model receive
% no information.  It doesn't seem like that would be a minima, but still worth looking at.)

% This means h_1 = O_1 * o_1 = (1,0,0)
% Ands since h_2 = (0,1,0), similarly
% Then T_1 must be a permutation matrix:
% (0,0,1)
% (1,0,0)
% (0,1,0)

% In inductive GPT, we have been feeding 3 word sentences to 3 layer models, but this HMM needs 4 words
% to constrain 3 layers.  We should look at that.  If we slid across the data we would transmit revolving data like
% that.  But right now, if we slid across the data, given that we marginalize out position, we would
% wash out all learning whatsoever.  An argument for needing position.

% - furthermore we are marginalizing out position, let's look at that.  If we had a 
% vocab (a,b,c,d,e), then for a sentence:

% [a, b, d] when we marginalize out position, then our data looks like this
% (our word_log_prob_tensors):

% o_1 = o_2 = o_3 = (1,1,0,1,0)

% and for another sentence a, c, e we get:

% o_1 = o_2 = o_3 = (1,0,1,0,1)

% If our observation matrix is 
% (1/5, 1/5, 1/5, 1/5, 1/5; 
%  1/5, 1/5, 1/5, 1/5, 1/5)

% h_1 = h_2 = (3/5, 3/5) = norm => (1/2, 1/2)
