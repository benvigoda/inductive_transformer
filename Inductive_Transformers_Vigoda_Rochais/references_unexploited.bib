%%% Attempting to understand what is going on in current SOA models

Untangling tradeoffs between recurrence and self-attention in artificial neural networks
https://scholar.google.com/scholar?hl=en&q=Kerg+G%2C+Kanuparthi+B%2C+Goyal+A%2C+Goyette+K%2C+Bengio+Y%2C+Lajoie+G.+2020+Untangling+tradeoffs+between+recurrence+and+self-attention+in+neural+networks.+%28http%3A%2F%2Farxiv.org%2Fabs%2F2006.09471%29

@inproceedings{luo-glass-2023-logic,
    title = "Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning",
    author = "Luo, Hongyin  and
      Glass, James",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.89",
    pages = "1243--1254",
    abstract = "Due to their similarity-based learning objectives, pretrained sentence encoders often internalize stereotypical assumptions that reflect the social biases that exist within their training corpora. In this paper, we describe several kinds of stereotypes concerning different communities that are present in popular sentence representation models, including pretrained next sentence prediction and contrastive sentence representation models. We compare such models to textual entailment models that learn language logic for a variety of downstream language understanding tasks. By comparing strong pretrained models based on text similarity with textual entailment learning, we conclude that the explicit logic learning with textual entailment can significantly reduce bias and improve the recognition of social communities, without an explicit de-biasing process.",
}


%https://www.nature.com/articles/s41467-023-36583-0
@article{article,
author = {Johnston, W. and Fusi, Stefano},
year = {2023},
month = {02},
pages = {},
title = {Abstract representations emerge naturally in neural networks trained to perform multiple tasks},
volume = {14},
journal = {Nature Communications},
doi = {10.1038/s41467-023-36583-0}
}

%RAG
@misc{zhao2023retrieving,
      title={Retrieving Multimodal Information for Augmented Generation: A Survey}, 
      author={Ruochen Zhao and Hailin Chen and Weishi Wang and Fangkai Jiao and Xuan Long Do and Chengwei Qin and Bosheng Ding and Xiaobao Guo and Minzhi Li and Xingxuan Li and Shafiq Joty},
      year={2023},
      eprint={2303.10868},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{kerg2020untangling,
  title={Untangling tradeoffs between recurrence and self-attention in artificial neural networks},
  author={Kerg, Giancarlo and Kanuparthi, Bhargav and ALIAS PARTH GOYAL, Anirudh Goyal and Goyette, Kyle and Bengio, Yoshua and Lajoie, Guillaume},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19443--19454},
  year={2020}
}


@misc{fu2023decoderonly,
      title={Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder}, 
      author={Zihao Fu and Wai Lam and Qian Yu and Anthony Man-Cho So and Shengding Hu and Zhiyuan Liu and Nigel Collier},
      year={2023},
      eprint={2304.04052},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

