\section{cool citations that haven't found a spot yet}

A comprehensive survey of graph neural networks
\cite{DBLP:journals/corr/abs-1901-00596}



mixup: Beyond Empirical Risk Minimization
Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.
\cite{DBLP:journals/corr/abs-1710-09412}

\cite{Tom Gruber papers}
@article{gruber1993translation,
  title={A translation approach to portable ontology specifications},
  author={Gruber, Thomas R},
  journal={Knowledge acquisition},
  volume={5},
  number={2},
  pages={199--220},
  year={1993},
  publisher={Elsevier}
}
@article{gruber1995toward,
  title={Toward principles for the design of ontologies used for knowledge sharing?},
  author={Gruber, Thomas R},
  journal={International journal of human-computer studies},
  volume={43},
  number={5-6},
  pages={907--928},
  year={1995},
  publisher={Elsevier}
}
@article{gruber2013nature,
  title={Nature, nurture, and knowledge acquisition},
  author={Gruber, Thomas R},
  journal={International journal of human-computer studies},
  volume={71},
  number={2},
  pages={191--194},
  year={2013},
  publisher={Elsevier}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}


@misc{mittal2022modular,
      title={Is a Modular Architecture Enough?}, 
      author={Sarthak Mittal and Yoshua Bengio and Guillaume Lajoie},
      year={2022},
      eprint={2206.02713},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{DBLP:journals/corr/abs-2011-15091,
  author       = {Anirudh Goyal and
                  Yoshua Bengio},
  title        = {Inductive Biases for Deep Learning of Higher-Level Cognition},
  journal      = {CoRR},
  volume       = {abs/2011.15091},
  year         = {2020},
  url          = {https://arxiv.org/abs/2011.15091},
  eprinttype    = {arXiv},
  eprint       = {2011.15091},
  timestamp    = {Tue, 01 Dec 2020 14:59:59 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2011-15091.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2103-00336,
  author       = {Alex Lamb and
                  Di He and
                  Anirudh Goyal and
                  Guolin Ke and
                  Chien{-}Feng Liao and
                  Mirco Ravanelli and
                  Yoshua Bengio},
  title        = {Transformers with Competitive Ensembles of Independent Mechanisms},
  journal      = {CoRR},
  volume       = {abs/2103.00336},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.00336},
  eprinttype    = {arXiv},
  eprint       = {2103.00336},
  timestamp    = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-00336.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2007-14062,
  author       = {Manzil Zaheer and
                  Guru Guruganesh and
                  Avinava Dubey and
                  Joshua Ainslie and
                  Chris Alberti and
                  Santiago Onta{\~{n}}{\'{o}}n and
                  Philip Pham and
                  Anirudh Ravula and
                  Qifan Wang and
                  Li Yang and
                  Amr Ahmed},
  title        = {Big Bird: Transformers for Longer Sequences},
  journal      = {CoRR},
  volume       = {abs/2007.14062},
  year         = {2020},
  url          = {https://arxiv.org/abs/2007.14062},
  eprinttype    = {arXiv},
  eprint       = {2007.14062},
  timestamp    = {Mon, 03 Aug 2020 14:32:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2007-14062.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2009-06732,
  author       = {Yi Tay and
                  Mostafa Dehghani and
                  Dara Bahri and
                  Donald Metzler},
  title        = {Efficient Transformers: {A} Survey},
  journal      = {CoRR},
  volume       = {abs/2009.06732},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.06732},
  eprinttype    = {arXiv},
  eprint       = {2009.06732},
  timestamp    = {Fri, 18 Sep 2020 15:17:35 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-06732.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inbook{10.5555/779343.779352,
author = {Yedidia, Jonathan S. and Freeman, William T. and Weiss, Yair},
title = {Understanding Belief Propagation and Its Generalizations},
year = {2003},
isbn = {1558608117},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {"Inference" problems arise in statistical physics, computer vision, error-correcting coding theory, and AI. We explain the principles behind the belief propagation (BP) algorithm, which is an efficient way to solve inference problems based on passing local messages. We develop a unified approach, with examples, notation, and graphical models borrowed from the relevant disciplines.We explain the close connection between the BP algorithm and the Bethe approximation of statistical physics. In particular, we show that BP can only converge to a fixed point that is also a stationary point of the Bethe approximation to the free energy. This result helps explaining the successes of the BP algorithm and enables connections to be made with variational approaches to approximate inference.The connection of BP with the Bethe approximation also suggests a way to construct new message-passing algorithms based on improvements to Bethe's approximation introduced Kikuchi and others. The new generalized belief propagation (GBP) algorithms are significantly more accurate than ordinary BP for some problems. We illustrate how to construct GBP algorithms with a detailed example.},
booktitle = {Exploring Artificial Intelligence in the New Millennium},
pages = {239–269},
numpages = {31}
}

@misc{dai2019transformerxl,
      title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}, 
      author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
      year={2019},
      eprint={1901.02860},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hernandez2023inspecting,
      title={Inspecting and Editing Knowledge Representations in Language Models}, 
      author={Evan Hernandez and Belinda Z. Li and Jacob Andreas},
      year={2023},
      eprint={2304.00740},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{meng2023massediting,
      title={Mass-Editing Memory in a Transformer}, 
      author={Kevin Meng and Arnab Sen Sharma and Alex Andonian and Yonatan Belinkov and David Bau},
      year={2023},
      eprint={2210.07229},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2023easyedit,
      title={EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models}, 
      author={Peng Wang and Ningyu Zhang and Xin Xie and Yunzhi Yao and Bozhong Tian and Mengru Wang and Zekun Xi and Siyuan Cheng and Kangwei Liu and Guozhou Zheng and Huajun Chen},
      year={2023},
      eprint={2308.07269},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{sartran2022transformer,
  title={Transformer grammars: Augmenting transformer language models with syntactic inductive biases at scale},
  author={Sartran, Laurent and Barrett, Samuel and Kuncoro, Adhiguna and Stanojevi{\'c}, Milo{\v{s}} and Blunsom, Phil and Dyer, Chris},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={1423--1439},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}


\cite{ruis2020benchmark}
A benchmark for systematic generalization in grounded language understanding

@book{Lerdahl1983,
  added-at = {2011-03-27T17:20:41.000+0200},
  address = {Cambridge. MA},
  author = {Lerdahl, Fred and Jackendoff, Ray},
  biburl = {https://www.bibsonomy.org/bibtex/27fa19545956a672507f147a39677bd2a/yevb0},
  booktitle = {A generative theory of tonal music},
  file = {Lerdahl, Jackendoff_1983_A generative theory of tonal music.zip:Lerdahl, Jackendoff_1983_A generative theory of tonal music.zip:ZIP},
  interhash = {1921dae625db785143d493cf15e46070},
  intrahash = {7fa19545956a672507f147a39677bd2a},
  isbn = {0262120941},
  keywords = {harmony,language,music,rhythm,syntax},
  mendeley-tags = {harmony,language,music,rhythm,syntax},
  publisher = {The MIT Press},
  timestamp = {2011-03-27T17:20:58.000+0200},
  title = {A generative theory of tonal music},
  year = 1983
}

@misc{candes2004robust,
      title={Robust Uncertainty Principles: Exact Signal Reconstruction from Highly Incomplete Frequency Information}, 
      author={Emmanuel Candes and Justin Romberg and Terence Tao},
      year={2004},
      eprint={math/0409186},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}


@article{Blair1957,
  title={Passing of a great mind},
  author={Blair Jr, Clay},
  journal={Life},
  volume={25},
  pages={96},
  year={1957}
}

@misc{rogers2020primer,
      title={A Primer in BERTology: What we know about how BERT works}, 
      author={Anna Rogers and Olga Kovaleva and Anna Rumshisky},
      year={2020},
      eprint={2002.12327},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2021pay,
      title={Pay Attention to MLPs}, 
      author={Hanxiao Liu and Zihang Dai and David R. So and Quoc V. Le},
      year={2021},
      eprint={2105.08050},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{Macrae1992,
  title={John von Neumann: The Scientific Genius Who Pioneered the Modern Computer, Game Theory, Nuclear Deterrence, and Much More.},
  author={Macrae, Norman},
  year={1992},
  publisher={Pantheon Press}
}

@book{Redei2005,
  title={John von Neumann selected letters},
  author={John Von Neumann and Miklos Redei},
  year={2005},
  publisher={American Mathematical Society, Providence, R.I.,}
}


@book{Schneider2015,
  title={Invitation to Computer Science},
  author={Schneider, Gersting \& Brinkman},
  page={28},
  year={2015},
  publisher={Boston: Cengage Learning}
}



@book{Henderson2007,
  title={Mathematics: Powerful Patterns Into Nature and Society},
  author={Henderson, Harry},
  page={30},
  year={2007},
  publisher={New York: Chelsea House}
}


@misc{merlin2022language,
      title={Language models and brain alignment: beyond word-level semantics and prediction}, 
      author={Gabriele Merlin and Mariya Toneva},
      year={2022},
      eprint={2212.00596},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{veres2022large,
      title={Large Language Models are not Models of Natural Language: they are Corpus Models}, 
      author={Csaba Veres},
      year={2022},
      eprint={2112.07055},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{marcus2023sentence,
      title={A Sentence is Worth a Thousand Pictures: Can Large Language Models Understand Human Language?}, 
      author={Gary Marcus and Evelina Leivada and Elliot Murphy},
      year={2023},
      eprint={2308.00109},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@phdthesis{murphy2002dynamic,
  added-at = {2017-03-30T22:59:50.000+0200},
  author = {Murphy, Kevin P},
  biburl = {https://www.bibsonomy.org/bibtex/23edd62101afabda26c12b9b235d65bb6/becker},
  interhash = {f1eb8d23313c740e97b4f0fbaca0df28},
  intrahash = {3edd62101afabda26c12b9b235d65bb6},
  keywords = {ar auto chain citedby:scholar:count:2586 citedby:scholar:timestamp:2017-3-30 diss hidden inthesis markov mixedtrails regression},
  school = {University of California, Berkeley},
  timestamp = {2017-12-20T17:57:45.000+0100},
  title = {Dynamic bayesian networks: Representation, inference and learning},
  year = 2002
}


@misc{malik2021generative,
      title={Generative Grading: Near Human-level Accuracy for Automated Feedback on Richly Structured Problems}, 
      author={Ali Malik and Mike Wu and Vrinda Vasavada and Jinpeng Song and Madison Coots and John Mitchell and Noah Goodman and Chris Piech},
      year={2021},
      eprint={1905.09916},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@article{welling2019,
  title={Do we still need models or just more data and compute},
  author={Welling, Max},
  journal={University of Amsterdam, April},
  volume={20},
  year={2019}
}


@inproceedings{ge-etal-2023-entailment,
    title = "Entailment as Robust Self-Learner",
    author = "Ge, Jiaxin  and
      Luo, Hongyin  and
      Kim, Yoon  and
      Glass, James",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.772",
    doi = "10.18653/v1/2023.acl-long.772",
    pages = "13803--13817",
    abstract = "Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU tasks as contextual entailment. This approach improves the zero-shot adaptation of pretrained entailment models. Secondly, we notice that self-training entailment-based models with unlabeled data can significantly improve the adaptation performance on downstream tasks. To achieve more stable improvement, we propose the Simple Pseudo-Label Editing (SimPLE) algorithm for better pseudo-labeling quality in self-training. We also found that both pretrained entailment-based models and the self-trained models are robust against adversarial evaluation data. Experiments on binary and multi-class classification tasks show that SimPLE leads to more robust self-training results, indicating that the self-trained entailment models are more efficient and trustworthy than large language models on language understanding tasks.",
}

@misc{frankle2019lottery,
      title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}, 
      author={Jonathan Frankle and Michael Carbin},
      year={2019},
      eprint={1803.03635},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Mitchell_2023,
	doi = {10.1073/pnas.2215907120},
	url = {https://doi.org/10.1073%2Fpnas.2215907120},
	year = 2023,
	month = {mar},
	publisher = {Proceedings of the National Academy of Sciences},
	volume = {120},
	number = {13},
	author = {Melanie Mitchell and David C. Krakauer},
	title = {The debate over understanding in {AI}'s large language models},
	journal = {Proceedings of the National Academy of Sciences}
}

Judea Pearl canonical paper on the Do Calculus that is most cited and led to his Turing award
@article{pearl1995causal,
  title={Causal diagrams for empirical research},
  author={Pearl, Judea},
  journal={Biometrika},
  volume={82},
  number={4},
  pages={669--688},
  year={1995},
  publisher={Oxford University Press}
}

@book{pearl1988probabilistic,
  title={Probabilistic reasoning in intelligent systems: networks of plausible inference},
  author={Pearl, Judea},
  year={1988},
  publisher={Morgan kaufmann}
}

@article{lu2022unified,
  title={Unified deep learning model for multitask reaction predictions with explanation},
  author={Lu, Jieyu and Zhang, Yingkai},
  journal={Journal of Chemical Information and Modeling},
  volume={62},
  number={6},
  pages={1376--1387},
  year={2022},
  publisher={ACS Publications}
}

@book{koller2009probabilistic,
  title={Probabilistic graphical models: principles and techniques},
  author={Koller, Daphne and Friedman, Nir},
  year={2009},
  publisher={MIT press}
}

discusses hierarchical compositionality of concepts
@article{lake2017building,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={Behavioral and brain sciences},
  volume={40},
  pages={e253},
  year={2017},
  publisher={Cambridge University Press}
}

@misc{goodman2014church,
      title={Church: a language for generative models}, 
      author={Noah Goodman and Vikash Mansinghka and Daniel M. Roy and Keith Bonawitz and Joshua B. Tenenbaum},
      year={2014},
      eprint={1206.3255},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}
@article{chomsky1956three,
  title={Three models for the description of language},
  author={Chomsky, Noam},
  journal={IRE Transactions on information theory},
  volume={2},
  number={3},
  pages={113--124},
  year={1956},
  publisher={IEEE}
}

on curiosity to resolve inadequacy in conceptual models
@article{schmidhuber2010formal,
  title={Formal theory of creativity, fun, and intrinsic motivation (1990--2010)},
  author={Schmidhuber, J{\"u}rgen},
  journal={IEEE transactions on autonomous mental development},
  volume={2},
  number={3},
  pages={230--247},
  year={2010},
  publisher={Ieee}
}

@inproceedings{mazzaglia2022curiosity,
  title={Curiosity-driven exploration via latent bayesian surprise},
  author={Mazzaglia, Pietro and Catal, Ozan and Verbelen, Tim and Dhoedt, Bart},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={7},
  pages={7752--7760},
  year={2022}
}

@article{chen2022redeeming,
  title={Redeeming intrinsic rewards via constrained optimization},
  author={Chen, Eric and Hong, Zhang-Wei and Pajarinen, Joni and Agrawal, Pulkit},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4996--5008},
  year={2022}
}

discussion of self-supervision methods involving introspection on perturbations
@article{liu2021self,
  title={Self-supervised learning: Generative or contrastive},
  author={Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Mian, Li and Wang, Zhaoyu and Zhang, Jing and Tang, Jie},
  journal={IEEE transactions on knowledge and data engineering},
  volume={35},
  number={1},
  pages={857--876},
  year={2021},
  publisher={IEEE}
}

meta-learning agents that can introspect/adapt to new tasks
@inproceedings{oh2017zero,
  title={Zero-shot task generalization with multi-task deep reinforcement learning},
  author={Oh, Junhyuk and Singh, Satinder and Lee, Honglak and Kohli, Pushmeet},
  booktitle={International Conference on Machine Learning},
  pages={2661--2670},
  year={2017},
  organization={PMLR}
}

@misc{shinn2023reflexion,
      title={Reflexion: Language Agents with Verbal Reinforcement Learning}, 
      author={Noah Shinn and Federico Cassano and Beck Labash and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
      year={2023},
      eprint={2303.11366},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@book{hrbacek2017introduction,
  title={Introduction to set theory, revised and expanded},
  author={Hrbacek, Karel and Jech, Thomas},
  year={2017},
  publisher={Crc Press}
}

few-shot classification via self-supervision
@inproceedings{tian2020rethinking,
  title={Rethinking few-shot image classification: a good embedding is all you need?},
  author={Tian, Yonglong and Wang, Yue and Krishnan, Dilip and Tenenbaum, Joshua B and Isola, Phillip},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XIV 16},
  pages={266--282},
  year={2020},
  organization={Springer}
}


curiosity as seeking to improve conceptual model
@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={2778--2787},
  year={2017},
  organization={PMLR}
}

Bengio paper on inductive bias
@article{goyal2022inductive,
  title={Inductive biases for deep learning of higher-level cognition},
  author={Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the Royal Society A},
  volume={478},
  number={2266},
  pages={20210068},
  year={2022},
  publisher={The Royal Society}
}

@article{frank2023bridging,
  title={Bridging the data gap between children and large language models},
  author={Frank, Michael C},
  journal={Trends in Cognitive Sciences},
  year={2023},
  publisher={Elsevier}
}

Talk at MIT by Sam Altman, CEO of OpenAI, Wired Article reported it, not sure best way to cite it
@article{knight2023openai,
  title={OpenAI’s CEO says the age of giant AI models is already over},
  author={Knight, Will},
  journal={Wired, April 17th},
  year={2023}
}
@misc{wei2022emergent,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

original RLHF paper
@inproceedings{thomaz2006reinforcement,
  title={Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance},
  author={Thomaz, Andrea Lockerd and Breazeal, Cynthia and others},
  booktitle={Aaai},
  volume={6},
  pages={1000--1005},
  year={2006},
  organization={Boston, MA}
}

paper on scaling of compute costs has been increasing 10x every year for 10 years 
@online{amodei2018ai,
author = {Dario Amodei and Danny Hernandez},
title = {AI and Compute},
year = 2018,
url = {https://openai.com/research/ai-and-compute},
urldate = {2023-02-27},
organization = {OpenAI}
}

is there a famous go-to biography?
@book{herrmann1999helen,
  title={Helen Keller: a life},
  author={Herrmann, Dorothy},
  year={1999},
  publisher={University of Chicago Press}
}

is there some kind of scaling paper about how adding video impacts a foundation model?
@article{DBLP:journals/corr/abs-1904-01766,
  author       = {Chen Sun and
                  Austin Myers and
                  Carl Vondrick and
                  Kevin Murphy and
                  Cordelia Schmid},
  title        = {VideoBERT: {A} Joint Model for Video and Language Representation Learning},
  journal      = {CoRR},
  volume       = {abs/1904.01766},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.01766},
  eprinttype    = {arXiv},
  eprint       = {1904.01766},
  timestamp    = {Wed, 21 Oct 2020 08:21:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-01766.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

This is analogous to tutoring children, but children without significant tutoring are still able to learn very effectively. How do we cite this?
@article{gopnik2020childhood,
  title={Childhood as a solution to explore--exploit tensions},
  author={Gopnik, Alison},
  journal={Philosophical Transactions of the Royal Society B},
  volume={375},
  number={1803},
  pages={20190502},
  year={2020},
  publisher={The Royal Society}
}

@inproceedings{clark-etal-2019-bert,
    title = "What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4828",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286",
    abstract = "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT{'}s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT{'}s attention.",
}

@article{peterson2019embracing,
  title={Embracing curiosity eliminates the exploration-exploitation dilemma},
  author={Peterson, Erik J and Verstynen, Timothy D and Yan, Xuan and Calcini, Niccolo and Safavi, Payam and Ak, Asli and Kole, Koen and Zeldenrust, Fleur and Celikel, Tansu and Fan, Yuanchan and others},
  year={2019}
}

@book{gopnik1999scientist,
  title={The scientist in the crib: Minds, brains, and how children learn.},
  author={Gopnik, Alison and Meltzoff, Andrew N and Kuhl, Patricia K},
  year={1999},
  publisher={William Morrow \& Co}
}





@article{DBLP:journals/corr/abs-2103-10334,
  author       = {Matthew B. A. McDermott and
                  Brendan Yap and
                  Peter Szolovits and
                  Marinka Zitnik},
  title        = {Rethinking Relational Encoding in Language Model: Pre-Training for
                  General Sequences},
  journal      = {CoRR},
  volume       = {abs/2103.10334},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.10334},
  eprinttype    = {arXiv},
  eprint       = {2103.10334},
  timestamp    = {Wed, 24 Mar 2021 15:50:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-10334.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{NEURIPS2019_9015,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{mackay2003information,
  title={Information theory, inference and learning algorithms},
  author={MacKay, David JC},
  year={2003},
  publisher={Cambridge university press}
}

From DeepMind: Formal Algorithms for Transformers
This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms.
@misc{phuong2022formal,
      title={Formal Algorithms for Transformers}, 
      author={Mary Phuong and Marcus Hutter},
      year={2022},
      eprint={2207.09238},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

MUST CITE: Very pedagogical paper: A Survey of Transformers
@article{DBLP:journals/corr/abs-2106-04554,
  author       = {Tianyang Lin and
                  Yuxin Wang and
                  Xiangyang Liu and
                  Xipeng Qiu},
  title        = {A Survey of Transformers},
  journal      = {CoRR},
  volume       = {abs/2106.04554},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.04554},
  eprinttype    = {arXiv},
  eprint       = {2106.04554},
  timestamp    = {Fri, 11 Jun 2021 11:04:16 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-04554.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.5555/3327757.3327820,
author = {Belbute-Peres, Filipe de A. and Smith, Kevin A. and Allen, Kelsey R. and Tenenbaum, Joshua B. and Kolter, J. Zico},
title = {End-to-End Differentiable Physics for Learning and Control},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a differentiable physics engine that can be integrated as a module in deep neural networks for end-to-end learning. As a result, structured physics knowledge can be embedded into larger systems, allowing them, for example, to match observations by performing precise simulations, while achieves high sample efficiency. Specifically, in this paper we demonstrate how to perform backpropagation analytically through a physical simulator defined via a linear complementarity problem. Unlike traditional finite difference methods, such gradients can be computed analytically, which allows for greater flexibility of the engine. Through experiments in diverse domains, we highlight the system's ability to learn physical parameters from data, efficiently match and simulate observed visual behavior, and readily enable control via gradient-based planning methods. Code for the engine and experiments is included with the paper.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {7178–7189},
numpages = {12},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{geva-etal-2021-transformer,
    title = "Transformer Feed-Forward Layers Are Key-Value Memories",
    author = "Geva, Mor  and
      Schuster, Roei  and
      Berant, Jonathan  and
      Levy, Omer",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.446",
    doi = "10.18653/v1/2021.emnlp-main.446",
    pages = "5484--5495",
    abstract = "Feed-forward layers constitute two-thirds of a transformer model{'}s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys{'} input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model{'}s layers via residual connections to produce the final output distribution.",
}

% Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space
@misc{geva2022transformer,
      title={Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space}, 
      author={Mor Geva and Avi Caciularu and Kevin Ro Wang and Yoav Goldberg},
      year={2022},
      eprint={2203.14680},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Large Language Models Converge on Brain-Like
Word Representations
@misc{li2023large,
      title={Large Language Models Converge on Brain-Like Word Representations}, 
      author={Jiaang Li and Antonia Karamolegkou and Yova Kementchedjhieva and Mostafa Abdou and Sune Lehmann and Anders Søgaard},
      year={2023},
      eprint={2306.01930},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{
doi:10.1073/pnas.1907375117,
author = {David Bau  and Jun-Yan Zhu  and Hendrik Strobelt  and Agata Lapedriza  and Bolei Zhou  and Antonio Torralba },
title = {Understanding the role of individual units in a deep neural network},
journal = {Proceedings of the National Academy of Sciences},
volume = {117},
number = {48},
pages = {30071-30078},
year = {2020},
doi = {10.1073/pnas.1907375117},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1907375117},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1907375117},
abstract = {Deep neural networks excel at finding hierarchical representations that solve complex tasks over large datasets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.}}


@article{BenVigodaThesis,
  title={Analog logic: Continuous-time analog circuits for statistical signal processing},
  author={Vigoda, Benjamin},
  journal={Online] Sep},
  year={2003},
  publisher={Citeseer}
}

@article{welling2007product,
  title={Product of experts},
  author={Welling, Max},
  journal={Scholarpedia},
  volume={2},
  number={10},
  pages={3879},
  year={2007}
}

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@Book{GoodBengCour16,
  Title                    = {Deep Learning},
  Author                   = {Ian J. Goodfellow and Yoshua Bengio and Aaron Courville},
  Publisher                = {MIT Press},
  Year                     = {2016},

  Address                  = {Cambridge, MA, USA},
  Note                     = { \url{http://www.deeplearningbook.org} }
}


@article{DBLP:journals/corr/abs-1712-09913,
  author       = {Hao Li and
                  Zheng Xu and
                  Gavin Taylor and
                  Tom Goldstein},
  title        = {Visualizing the Loss Landscape of Neural Nets},
  journal      = {CoRR},
  volume       = {abs/1712.09913},
  year         = {2017},
  url          = {http://arxiv.org/abs/1712.09913},
  eprinttype    = {arXiv},
  eprint       = {1712.09913},
  timestamp    = {Thu, 16 May 2019 13:19:49 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1712-09913.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1902-02476,
  author       = {Wesley J. Maddox and
                  Timur Garipov and
                  Pavel Izmailov and
                  Dmitry P. Vetrov and
                  Andrew Gordon Wilson},
  title        = {A Simple Baseline for Bayesian Uncertainty in Deep Learning},
  journal      = {CoRR},
  volume       = {abs/1902.02476},
  year         = {2019},
  url          = {http://arxiv.org/abs/1902.02476},
  eprinttype    = {arXiv},
  eprint       = {1902.02476},
  timestamp    = {Tue, 12 Apr 2022 21:46:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1902-02476.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/nature/HuthHGTG16,
  author       = {Alexander G. Huth and
                  Wendy A. de Heer and
                  Thomas L. Griffiths and
                  Fr{\'{e}}d{\'{e}}ric E. Theunissen and
                  Jack L. Gallant},
  title        = {Natural speech reveals the semantic maps that tile human cerebral
                  cortex},
  journal      = {Nat.},
  volume       = {532},
  number       = {7600},
  pages        = {453--458},
  year         = {2016},
  url          = {https://doi.org/10.1038/nature17637},
  doi          = {10.1038/nature17637},
  timestamp    = {Thu, 28 Jul 2022 09:56:44 +0200},
  biburl       = {https://dblp.org/rec/journals/nature/HuthHGTG16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1901-00596,
  author       = {Zonghan Wu and
                  Shirui Pan and
                  Fengwen Chen and
                  Guodong Long and
                  Chengqi Zhang and
                  Philip S. Yu},
  title        = {A Comprehensive Survey on Graph Neural Networks},
  journal      = {CoRR},
  volume       = {abs/1901.00596},
  year         = {2019},
  url          = {http://arxiv.org/abs/1901.00596},
  eprinttype    = {arXiv},
  eprint       = {1901.00596},
  timestamp    = {Thu, 31 Jan 2019 13:52:49 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1901-00596.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1710-09412,
  author       = {Hongyi Zhang and
                  Moustapha Ciss{\'{e}} and
                  Yann N. Dauphin and
                  David Lopez{-}Paz},
  title        = {mixup: Beyond Empirical Risk Minimization},
  journal      = {CoRR},
  volume       = {abs/1710.09412},
  year         = {2017},
  url          = {http://arxiv.org/abs/1710.09412},
  eprinttype    = {arXiv},
  eprint       = {1710.09412},
  timestamp    = {Mon, 13 Aug 2018 16:47:14 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1710-09412.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{akyurek2020learning,
  title={Learning to recombine and resample data for compositional generalization},
  author={Aky{\"u}rek, Ekin and Aky{\"u}rek, Afra Feyza and Andreas, Jacob},
  journal={arXiv preprint arXiv:2010.03706},
  year={2020}
}


@article{ruis2020benchmark,
  title={A benchmark for systematic generalization in grounded language understanding},
  author={Ruis, Laura and Andreas, Jacob and Baroni, Marco and Bouchacourt, Diane and Lake, Brenden M},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={19861--19872},
  year={2020}
}


@article{hinton1984distributed,
  title={Distributed representations},
  author={Hinton, Geoffrey E},
  year={1984},
  publisher={Carnegie Mellon University}
}
