\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wang2023easyedit,meng2023massediting,hernandez2023inspecting}
\citation{pearl1995causal}
\citation{lu2022unified}
\citation{lake2017building,oh2017zero,shinn2023reflexion}
\citation{mazzaglia2022curiosity,chen2022redeeming,pearl1988probabilistic,peterson2019embracing}
\citation{amodei2018ai}
\citation{frank2023bridging}
\citation{knight2023openai,thomaz2006reinforcement}
\citation{gopnik1999scientist}
\citation{DBLP:journals/corr/abs-1904-01766}
\citation{herrmann1999helen}
\citation{goyal2022inductive}
\citation{mittal2022modular,DBLP:journals/corr/abs-2011-15091,DBLP:journals/corr/abs-2103-00336,gruber2013nature}
\citation{mackay2003information}
\citation{Redei2005}
\citation{Schneider2015,Henderson2007}
\citation{Macrae1992,Blair1957}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction and Prior Art}{1}{section.1}\protected@file@percent }
\citation{welling2019}
\citation{10.5555/3327757.3327820}
\citation{ge-etal-2023-entailment}
\citation{liu2021pay}
\citation{marcus2023sentence}
\citation{veres2022large}
\citation{doi:10.1073/pnas.1907375117}
\citation{clark-etal-2019-bert}
\citation{rogers2020primer}
\citation{clark-etal-2019-bert}
\citation{frankle2019lottery}
\citation{goyal2022inductive}
\citation{sartran2022transformer}
\@writefile{toc}{\contentsline {section}{\numberline {2}The Inductive Transformer Model}{3}{section.2}\protected@file@percent }
\newlabel{section:inductive-transformer-model}{{2}{3}{The Inductive Transformer Model}{section.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A single layer of the inductive transformer production represented as a factor graph.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:factor-graph-for-inductive-transformer}{{1}{3}{A single layer of the inductive transformer production represented as a factor graph}{figure.1}{}}
\citation{dai2019transformerxl}
\citation{hrbacek2017introduction}
\citation{gruber1993translation,gruber1995toward}
\citation{Lerdahl1983}
\citation{vaswani2017attention}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of Vanilla and Inductive Transformer Layers}}{5}{table.1}\protected@file@percent }
\newlabel{tab:rosetta}{{1}{5}{Comparison of Vanilla and Inductive Transformer Layers}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Inference in the Inductive Transformer}{6}{section.3}\protected@file@percent }
\newlabel{eq:rho_marginalized_out}{{1}{6}{Inference in the Inductive Transformer}{equation.3.1}{}}
\citation{NEURIPS2019_9015,kingma2017adam}
\@writefile{toc}{\contentsline {section}{\numberline {4}Illustrative Example}{7}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model weights and activations}{7}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Learned Weights in the Inductive Transformer. The learning is highly reproducible. In a hundred different learning runs, the variance of each learned weight is generally less than 1\%. The attention $\pi _Z$ weights are in white with black background while the token $\pi _T$ weights are black on white, next to their corresponding vocabulary words.}}{7}{figure.2}\protected@file@percent }
\newlabel{fig:result-weights}{{2}{7}{Learned Weights in the Inductive Transformer. The learning is highly reproducible. In a hundred different learning runs, the variance of each learned weight is generally less than 1\%. The attention $\pi _Z$ weights are in white with black background while the token $\pi _T$ weights are black on white, next to their corresponding vocabulary words}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Prompting and Generation}{7}{subsection.4.2}\protected@file@percent }
\citation{DBLP:journals/nature/HuthHGTG16,li2023large,geva-etal-2021-transformer,merlin2022language}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Identifiability}{8}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Controllability}{8}{subsection.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces After training, the model accurately reflects the training data.}}{8}{table.2}\protected@file@percent }
\citation{wei2022emergent}
\citation{akyurek2020learning}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Prompted generations from the model where we broke the connection between ``big'' and ``dog''.}}{9}{table.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{9}{section.5}\protected@file@percent }
\bibstyle{iclr2024_conference}
\bibdata{references}
\bibcite{akyurek2020learning}{{1}{2020}{{Aky{\"u}rek et~al.}}{{Aky{\"u}rek, Aky{\"u}rek, and Andreas}}}
\bibcite{amodei2018ai}{{2}{2018}{{Amodei \& Hernandez}}{{Amodei and Hernandez}}}
\bibcite{doi:10.1073/pnas.1907375117}{{3}{2020}{{Bau et~al.}}{{Bau, Zhu, Strobelt, Lapedriza, Zhou, and Torralba}}}
\bibcite{10.5555/3327757.3327820}{{4}{2018}{{Belbute-Peres et~al.}}{{Belbute-Peres, Smith, Allen, Tenenbaum, and Kolter}}}
\bibcite{Blair1957}{{5}{1957}{{Blair~Jr}}{{}}}
\bibcite{chen2022redeeming}{{6}{2022}{{Chen et~al.}}{{Chen, Hong, Pajarinen, and Agrawal}}}
\bibcite{clark-etal-2019-bert}{{7}{2019}{{Clark et~al.}}{{Clark, Khandelwal, Levy, and Manning}}}
\bibcite{dai2019transformerxl}{{8}{2019}{{Dai et~al.}}{{Dai, Yang, Yang, Carbonell, Le, and Salakhutdinov}}}
\bibcite{frank2023bridging}{{9}{2023}{{Frank}}{{}}}
\bibcite{frankle2019lottery}{{10}{2019}{{Frankle \& Carbin}}{{Frankle and Carbin}}}
\bibcite{ge-etal-2023-entailment}{{11}{2023}{{Ge et~al.}}{{Ge, Luo, Kim, and Glass}}}
\bibcite{geva-etal-2021-transformer}{{12}{2021}{{Geva et~al.}}{{Geva, Schuster, Berant, and Levy}}}
\bibcite{gopnik1999scientist}{{13}{1999}{{Gopnik et~al.}}{{Gopnik, Meltzoff, and Kuhl}}}
\bibcite{DBLP:journals/corr/abs-2011-15091}{{14}{2020}{{Goyal \& Bengio}}{{Goyal and Bengio}}}
\bibcite{goyal2022inductive}{{15}{2022}{{Goyal \& Bengio}}{{Goyal and Bengio}}}
\bibcite{gruber1993translation}{{16}{1993}{{Gruber}}{{}}}
\bibcite{gruber1995toward}{{17}{1995}{{Gruber}}{{}}}
\bibcite{gruber2013nature}{{18}{2013}{{Gruber}}{{}}}
\bibcite{Henderson2007}{{19}{2007}{{Henderson}}{{}}}
\bibcite{hernandez2023inspecting}{{20}{2023}{{Hernandez et~al.}}{{Hernandez, Li, and Andreas}}}
\bibcite{herrmann1999helen}{{21}{1999}{{Herrmann}}{{}}}
\bibcite{hrbacek2017introduction}{{22}{2017}{{Hrbacek \& Jech}}{{Hrbacek and Jech}}}
\bibcite{DBLP:journals/nature/HuthHGTG16}{{23}{2016}{{Huth et~al.}}{{Huth, de~Heer, Griffiths, Theunissen, and Gallant}}}
\bibcite{kingma2017adam}{{24}{2017}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{knight2023openai}{{25}{2023}{{Knight}}{{}}}
\bibcite{lake2017building}{{26}{2017}{{Lake et~al.}}{{Lake, Ullman, Tenenbaum, and Gershman}}}
\bibcite{DBLP:journals/corr/abs-2103-00336}{{27}{2021}{{Lamb et~al.}}{{Lamb, He, Goyal, Ke, Liao, Ravanelli, and Bengio}}}
\bibcite{Lerdahl1983}{{28}{1983}{{Lerdahl \& Jackendoff}}{{Lerdahl and Jackendoff}}}
\bibcite{DBLP:journals/corr/abs-1712-09913}{{29}{2017}{{Li et~al.}}{{Li, Xu, Taylor, and Goldstein}}}
\bibcite{li2023large}{{30}{2023}{{Li et~al.}}{{Li, Karamolegkou, Kementchedjhieva, Abdou, Lehmann, and SÃ¸gaard}}}
\bibcite{liu2021pay}{{31}{2021}{{Liu et~al.}}{{Liu, Dai, So, and Le}}}
\bibcite{lu2022unified}{{32}{2022}{{Lu \& Zhang}}{{Lu and Zhang}}}
\bibcite{mackay2003information}{{33}{2003}{{MacKay}}{{}}}
\bibcite{Macrae1992}{{34}{1992}{{Macrae}}{{}}}
\bibcite{malik2021generative}{{35}{2021}{{Malik et~al.}}{{Malik, Wu, Vasavada, Song, Coots, Mitchell, Goodman, and Piech}}}
\bibcite{marcus2023sentence}{{36}{2023}{{Marcus et~al.}}{{Marcus, Leivada, and Murphy}}}
\bibcite{mazzaglia2022curiosity}{{37}{2022}{{Mazzaglia et~al.}}{{Mazzaglia, Catal, Verbelen, and Dhoedt}}}
\bibcite{meng2023massediting}{{38}{2023}{{Meng et~al.}}{{Meng, Sharma, Andonian, Belinkov, and Bau}}}
\bibcite{merlin2022language}{{39}{2022}{{Merlin \& Toneva}}{{Merlin and Toneva}}}
\bibcite{mittal2022modular}{{40}{2022}{{Mittal et~al.}}{{Mittal, Bengio, and Lajoie}}}
\bibcite{murphy2002dynamic}{{41}{2002}{{Murphy}}{{}}}
\bibcite{Redei2005}{{42}{2005}{{Neumann \& Redei}}{{Neumann and Redei}}}
\bibcite{oh2017zero}{{43}{2017}{{Oh et~al.}}{{Oh, Singh, Lee, and Kohli}}}
\bibcite{NEURIPS2019_9015}{{44}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}}}
\bibcite{pearl1988probabilistic}{{45}{1988}{{Pearl}}{{}}}
\bibcite{pearl1995causal}{{46}{1995}{{Pearl}}{{}}}
\bibcite{peterson2019embracing}{{47}{2019}{{Peterson et~al.}}{{Peterson, Verstynen, Yan, Calcini, Safavi, Ak, Kole, Zeldenrust, Celikel, Fan, et~al.}}}
\bibcite{rogers2020primer}{{48}{2020}{{Rogers et~al.}}{{Rogers, Kovaleva, and Rumshisky}}}
\bibcite{sartran2022transformer}{{49}{2022}{{Sartran et~al.}}{{Sartran, Barrett, Kuncoro, Stanojevi{\'c}, Blunsom, and Dyer}}}
\bibcite{Schneider2015}{{50}{2015}{{Schneider}}{{}}}
\bibcite{shinn2023reflexion}{{51}{2023}{{Shinn et~al.}}{{Shinn, Cassano, Labash, Gopinath, Narasimhan, and Yao}}}
\bibcite{DBLP:journals/corr/abs-1904-01766}{{52}{2019}{{Sun et~al.}}{{Sun, Myers, Vondrick, Murphy, and Schmid}}}
\bibcite{DBLP:journals/corr/abs-2009-06732}{{53}{2020}{{Tay et~al.}}{{Tay, Dehghani, Bahri, and Metzler}}}
\bibcite{thomaz2006reinforcement}{{54}{2006}{{Thomaz et~al.}}{{Thomaz, Breazeal, et~al.}}}
\bibcite{vaswani2017attention}{{55}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{veres2022large}{{56}{2022}{{Veres}}{{}}}
\bibcite{BenVigodaThesis}{{57}{2003}{{Vigoda}}{{}}}
\bibcite{wang2023easyedit}{{58}{2023}{{Wang et~al.}}{{Wang, Zhang, Xie, Yao, Tian, Wang, Xi, Cheng, Liu, Zheng, and Chen}}}
\bibcite{wei2022emergent}{{59}{2022}{{Wei et~al.}}{{Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang, Dean, and Fedus}}}
\bibcite{welling2019}{{60}{2019}{{Welling}}{{}}}
\bibcite{DBLP:journals/corr/abs-2007-14062}{{61}{2020}{{Zaheer et~al.}}{{Zaheer, Guruganesh, Dubey, Ainslie, Alberti, Onta{\~{n}}{\'{o}}n, Pham, Ravula, Wang, Yang, and Ahmed}}}
\citation{DBLP:journals/corr/abs-1712-09913}
\citation{malik2021generative}
\citation{pearl1988probabilistic,mackay2003information}
\@writefile{toc}{\contentsline {section}{\numberline {A}Connectivity in the Inductive Transformer}{14}{appendix.A}\protected@file@percent }
\newlabel{appendix:connectivity}{{A}{14}{Connectivity in the Inductive Transformer}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Residual Connections in Vanilla Transformer $\rightarrow $ Reduced Productions in the Inductive Transformer}{14}{subsection.A.1}\protected@file@percent }
\newlabel{appendix:residual-connection}{{A.1}{14}{Residual Connections in Vanilla Transformer $\rightarrow $ Reduced Productions in the Inductive Transformer}{subsection.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Connecting the Encoder to the Decoder}{14}{subsection.A.2}\protected@file@percent }
\newlabel{appendix:connecting-encoder-to-decoder}{{A.2}{14}{Connecting the Encoder to the Decoder}{subsection.A.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces How the Connectivity from Encoder to Decoder Differs in the Inductive Transformer.}}{14}{figure.3}\protected@file@percent }
\newlabel{fig:logical-encoder-decoder}{{3}{14}{How the Connectivity from Encoder to Decoder Differs in the Inductive Transformer}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Activation Functions}{15}{appendix.B}\protected@file@percent }
\newlabel{appendix:activation-functions}{{B}{15}{Activation Functions}{appendix.B}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Factor Graph For Inductive Transformer}}{15}{figure.4}\protected@file@percent }
\newlabel{fig-appendix:factor-graph-for-inductive-transformer}{{4}{15}{Factor Graph For Inductive Transformer}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}1. The Open-Closed Universe Factor}{16}{subsection.B.1}\protected@file@percent }
\newlabel{section:open-closed-universe}{{B.1}{16}{1. The Open-Closed Universe Factor}{subsection.B.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces One grey box in this figure corresponds to the grey box in figure\nobreakspace  {}\ref {fig:factor-graph-for-inductive-transformer} (a.) Open Universe Model: Every child node has a single parent node. Parents make categorical choices over existing children. A child that is activated has only one parent activating it. (b.) Open Universe Model: If two parent nodes would like to activate the same child, we must make a copy of that child. The two yellow rectangles represent the same concept. The open universe model must duplicate it in order to allow two parents to utilize it. (c.) Closed Universe Model: Duplicate concepts are merged into a single node. A child, therefore, can have more than one parent node. Since we do not have a potentially infinite layer width, this allows for re-use of limited closed universe resources, and allows for use of scalable closed-universe solvers such as back-propagation. To do the math correctly when a single child is simultaneously selected by more than one parent, we must use a conditional distribution where if one or more parents selects the child, then the child is activated. (d.) Closed Universe Model: We can make every layer the same width in order to allow for more routes through the model to explain away the data. This is equivalent to an open universe grammar with N children under the root, and then only N different types of grandchild node, and so forth.}}{16}{figure.5}\protected@file@percent }
\newlabel{fig-appendix:from-open-universe-to-closed-universe}{{5}{16}{One grey box in this figure corresponds to the grey box in figure~\ref {fig:factor-graph-for-inductive-transformer} (a.) Open Universe Model: Every child node has a single parent node. Parents make categorical choices over existing children. A child that is activated has only one parent activating it. (b.) Open Universe Model: If two parent nodes would like to activate the same child, we must make a copy of that child. The two yellow rectangles represent the same concept. The open universe model must duplicate it in order to allow two parents to utilize it. (c.) Closed Universe Model: Duplicate concepts are merged into a single node. A child, therefore, can have more than one parent node. Since we do not have a potentially infinite layer width, this allows for re-use of limited closed universe resources, and allows for use of scalable closed-universe solvers such as back-propagation. To do the math correctly when a single child is simultaneously selected by more than one parent, we must use a conditional distribution where if one or more parents selects the child, then the child is activated. (d.) Closed Universe Model: We can make every layer the same width in order to allow for more routes through the model to explain away the data. This is equivalent to an open universe grammar with N children under the root, and then only N different types of grandchild node, and so forth}{figure.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces This table represents the probabilities for some example states of an Open-Closed-Universe factor with two parents and one child. More details are provided in appendix \ref {appendix:Open-Closed-Universe}}}{16}{table.4}\protected@file@percent }
\newlabel{tab:open-universe-encoder}{{4}{16}{This table represents the probabilities for some example states of an Open-Closed-Universe factor with two parents and one child. More details are provided in appendix \ref {appendix:Open-Closed-Universe}}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}2. Open Closed Universe: Representing an open universe non-parametric model in a closed universe neural network with fixed layer width}{17}{subsection.B.2}\protected@file@percent }
\newlabel{appendix:Open-Closed-Universe}{{B.2}{17}{2. Open Closed Universe: Representing an open universe non-parametric model in a closed universe neural network with fixed layer width}{subsection.B.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Figure provided again for convenience. Again, one grey box in this figure corresponds to the grey box in figure\nobreakspace  {}\ref {fig-appendix:factor-graph-for-inductive-transformer} (a.) Open Universe Model: Every child node has a single parent node. Parents make categorical choices over existing children. A child that is activated has only one parent activating it. (b.) Open Universe Model: If two parent nodes would like to activate the same child, we must make a copy of that child. The two yellow rectangles represent the same concept. The open universe model must duplicate it in order to allow two parents to utilize it. (c.) Closed Universe Model: Duplicate concepts are merged into a single node. A child, therefore, can have more than one parent node. Since we do not have a potentially infinite layer width, this allows for re-use of limited closed universe resources, and allows for use of scalable closed-universe solvers such as back-propagation. To do the math correctly when a single child is simultaneously selected by more than one parent, we must use a conditional distribution where if one or more parents selects the child, then the child is activated. (d.) Closed Universe Model: We can make every layer the same width in order to allow for more routes through the model to explain away the data. This is equivalent to an open universe grammar with N children under the root, and then only N different types of grandchild node, and so forth.}}{17}{figure.6}\protected@file@percent }
\newlabel{fig:from-open-universe-to-closed-universe-appendix}{{6}{17}{Figure provided again for convenience. Again, one grey box in this figure corresponds to the grey box in figure~\ref {fig-appendix:factor-graph-for-inductive-transformer} (a.) Open Universe Model: Every child node has a single parent node. Parents make categorical choices over existing children. A child that is activated has only one parent activating it. (b.) Open Universe Model: If two parent nodes would like to activate the same child, we must make a copy of that child. The two yellow rectangles represent the same concept. The open universe model must duplicate it in order to allow two parents to utilize it. (c.) Closed Universe Model: Duplicate concepts are merged into a single node. A child, therefore, can have more than one parent node. Since we do not have a potentially infinite layer width, this allows for re-use of limited closed universe resources, and allows for use of scalable closed-universe solvers such as back-propagation. To do the math correctly when a single child is simultaneously selected by more than one parent, we must use a conditional distribution where if one or more parents selects the child, then the child is activated. (d.) Closed Universe Model: We can make every layer the same width in order to allow for more routes through the model to explain away the data. This is equivalent to an open universe grammar with N children under the root, and then only N different types of grandchild node, and so forth}{figure.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces This table represents unnormalized probabilities for some example states of an Open-Closed-Universe factor with two parents and one child. In the decoder, we will need the conditional probability of the child given all of the parents, $p(\text  {child}|\text  {parent}_0, \text  {parent}_1)$. In the encoder, we will need the conditional probability of one of the parents, given the child and the other parents, $p(\text  {parent}_0|\text  {child}, \text  {parent}_1)$. Both conditional distributions can be derived from this same table. In an open-universe model there would be two copies of the child, so that each parent has its own unique children, and therefore each child would have only one parent. When we combine multiple children into a single node with multiple parents, then that combined child should be active if one or more of its parents are active. This is what we do when we represent an open universe within a closed universe model. We represent allowed states with a probability of $1$. In this table $\text  {probability}=1$ states have the $\text  {child}=1$ when any of its parents are a $1$. Another valid state is to have the $\text  {child}=0$ when all parents are zeros. Disallowed states (rows) violate those rules and involve one or more parents being a $1$ while the $\text  {child}=0$. Disallowed states have a probability of $0$.}}{18}{table.5}\protected@file@percent }
\newlabel{tab:open-universe-encoder-appendix}{{5}{18}{This table represents unnormalized probabilities for some example states of an Open-Closed-Universe factor with two parents and one child. In the decoder, we will need the conditional probability of the child given all of the parents, $p(\text {child}|\text {parent}_0, \text {parent}_1)$. In the encoder, we will need the conditional probability of one of the parents, given the child and the other parents, $p(\text {parent}_0|\text {child}, \text {parent}_1)$. Both conditional distributions can be derived from this same table. In an open-universe model there would be two copies of the child, so that each parent has its own unique children, and therefore each child would have only one parent. When we combine multiple children into a single node with multiple parents, then that combined child should be active if one or more of its parents are active. This is what we do when we represent an open universe within a closed universe model. We represent allowed states with a probability of $1$. In this table $\text {probability}=1$ states have the $\text {child}=1$ when any of its parents are a $1$. Another valid state is to have the $\text {child}=0$ when all parents are zeros. Disallowed states (rows) violate those rules and involve one or more parents being a $1$ while the $\text {child}=0$. Disallowed states have a probability of $0$}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Bernoulli-Categorical}{19}{subsection.B.3}\protected@file@percent }
\newlabel{appendix:Bernoulli-Categorical}{{B.3}{19}{Bernoulli-Categorical}{subsection.B.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Each row in this table represents the probability of a joint state for a Categorical and two Bernoullis. In this example, the state of the Categorical can be either dog or cat. One Bernoulli is providing information about if there is a dog or not. A second Bernoulli tells us if we see a cat or not. The conditional distribution of the Categorical given both Bernoullis is $p(y_{\text  {Categorical}} \in \{\text  {dog, cat}\}|y^\text  {dog}_{\text  {Ber}} \in \{0,1\}, y^\text  {cat}_{\text  {Ber}}) \in \{0,1\})$. When the incoming dog-Bernoulli is a 1 and the incoming cat-Bernoulli is a 0, and the categorical is set to its dog state, then this joint state is allowed. We write the probability for this first row in the table as a 1, knowing that we will ultimately normalize the probabilities across all of the rows. Similarly we have a probability=1 in the second row where the cat-Bernoulli is 1, the dog-Bernoulli is 0, and the categorical is set to the cat state. However, as we see in rows 2 and 3, when the dog-Bernoulli and the cat-Bernoulli are both ones or both zeros, these are zero probability joint states. The Bernoulli-categorical factor obeys the constraint that dog and cat can't both be 1, nor can they both be 0. Therefore the probabilities in the last two rows are 0.}}{19}{table.6}\protected@file@percent }
\newlabel{tab:Bernoulli-categorical}{{6}{19}{Each row in this table represents the probability of a joint state for a Categorical and two Bernoullis. In this example, the state of the Categorical can be either dog or cat. One Bernoulli is providing information about if there is a dog or not. A second Bernoulli tells us if we see a cat or not. The conditional distribution of the Categorical given both Bernoullis is $p(y_{\text {Categorical}} \in \{\text {dog, cat}\}|y^\text {dog}_{\text {Ber}} \in \{0,1\}, y^\text {cat}_{\text {Ber}}) \in \{0,1\})$. When the incoming dog-Bernoulli is a 1 and the incoming cat-Bernoulli is a 0, and the categorical is set to its dog state, then this joint state is allowed. We write the probability for this first row in the table as a 1, knowing that we will ultimately normalize the probabilities across all of the rows. Similarly we have a probability=1 in the second row where the cat-Bernoulli is 1, the dog-Bernoulli is 0, and the categorical is set to the cat state. However, as we see in rows 2 and 3, when the dog-Bernoulli and the cat-Bernoulli are both ones or both zeros, these are zero probability joint states. The Bernoulli-categorical factor obeys the constraint that dog and cat can't both be 1, nor can they both be 0. Therefore the probabilities in the last two rows are 0}{table.6}{}}
\newlabel{eq:Bernoulli-categorical}{{17}{19}{Bernoulli-Categorical}{equation.B.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Encoder $\pi $}{20}{subsection.B.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces The allowed states in the $\pi _Z$ factor, and two examples of disallowed states. $\pi $ is an ``exactly-one'' constraint that states that $y=1$ if and only if $\DOTSB \sum@ \slimits@ _i z_i = 1$. In other words, only a single $z_i=1$ is allowed for $y$ to be equal to $1$}}{20}{table.7}\protected@file@percent }
\newlabel{tab:X1}{{7}{20}{The allowed states in the $\pi _Z$ factor, and two examples of disallowed states. $\pi $ is an ``exactly-one'' constraint that states that $y=1$ if and only if $\sum _i z_i = 1$. In other words, only a single $z_i=1$ is allowed for $y$ to be equal to $1$}{table.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Categorical-To-Bernoulli}{22}{subsection.B.5}\protected@file@percent }
\newlabel{appendix:Categorical-To-Bernoulli}{{B.5}{22}{Categorical-To-Bernoulli}{subsection.B.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Factor Graph for Categorical to Bernoullis Factor}}{22}{figure.7}\protected@file@percent }
\newlabel{fig:categorical-to-bernoulli-single}{{7}{22}{Factor Graph for Categorical to Bernoullis Factor}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Factor Graph for Categorical to Bernoulli Factor. The arrows indicate the direction of the messages.}}{22}{figure.8}\protected@file@percent }
\newlabel{fig:categorical-to-bernoullis}{{8}{22}{Factor Graph for Categorical to Bernoulli Factor. The arrows indicate the direction of the messages}{figure.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces }}{23}{table.8}\protected@file@percent }
\newlabel{tab:categorical-bernoulli}{{8}{23}{}{table.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.6}Encoder $\land $}{24}{subsection.B.6}\protected@file@percent }
\citation{BenVigodaThesis,murphy2002dynamic}
\newlabel{tab:open-universe-encoder-truth-table}{{B.6}{25}{Encoder $\land $}{subsection.B.6}{}}
\newlabel{eq:encoder-and}{{45}{25}{Encoder $\land $}{equation.B.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.7}Decoder $\land $}{25}{subsection.B.7}\protected@file@percent }
\newlabel{appendix:decoder-and}{{B.7}{25}{Decoder $\land $}{subsection.B.7}{}}
\newlabel{eq:decoder-and}{{46}{25}{Decoder $\land $}{equation.B.46}{}}
\citation{dai2019transformerxl}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.8}Decoder Token $\pi $}{26}{subsection.B.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Self-Attention in the Inductive Transformer}{26}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Vanilla Self-Attention}{26}{subsection.C.1}\protected@file@percent }
\newlabel{appendix:vanilla-self-attention}{{C.1}{26}{Vanilla Self-Attention}{subsection.C.1}{}}
\newlabel{eq:vanilla-self-attention}{{48}{26}{Vanilla Self-Attention}{equation.C.48}{}}
\citation{BenVigodaThesis}
\citation{DBLP:journals/corr/abs-2009-06732,DBLP:journals/corr/abs-2007-14062}
\citation{sartran2022transformer}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}A Generative Production for Self-Attention}{27}{subsection.C.2}\protected@file@percent }
\newlabel{eq:prob-position-generate}{{49}{27}{A Generative Production for Self-Attention}{equation.C.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Inference in the Inductive Transformer Attention Layer}{28}{subsection.C.3}\protected@file@percent }
\newlabel{eq-appendix:encoder-attention}{{50}{28}{Inference in the Inductive Transformer Attention Layer}{equation.C.50}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}$\land $ in the Log Probability Domain}{29}{appendix.D}\protected@file@percent }
\newlabel{appendix:log-probability-and}{{D}{29}{$\land $ in the Log Probability Domain}{appendix.D}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}From Indicator Vectors to Embedding Vectors}{29}{appendix.E}\protected@file@percent }
\newlabel{appendix:from-indicator-to-embedding}{{E}{29}{From Indicator Vectors to Embedding Vectors}{appendix.E}{}}
\newlabel{eq:arbitrary_factor}{{54}{30}{From Indicator Vectors to Embedding Vectors}{equation.E.54}{}}
\gdef \@abspage@last{30}
