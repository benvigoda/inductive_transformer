\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aky{\"u}rek et~al.(2020)Aky{\"u}rek, Aky{\"u}rek, and
  Andreas]{akyurek2020learning}
Ekin Aky{\"u}rek, Afra~Feyza Aky{\"u}rek, and Jacob Andreas.
\newblock Learning to recombine and resample data for compositional
  generalization.
\newblock \emph{arXiv preprint arXiv:2010.03706}, 2020.

\bibitem[Amodei \& Hernandez(2018)Amodei and Hernandez]{amodei2018ai}
Dario Amodei and Danny Hernandez.
\newblock Ai and compute, 2018.
\newblock URL \url{https://openai.com/research/ai-and-compute}.

\bibitem[Bau et~al.(2020)Bau, Zhu, Strobelt, Lapedriza, Zhou, and
  Torralba]{doi:10.1073/pnas.1907375117}
David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and
  Antonio Torralba.
\newblock Understanding the role of individual units in a deep neural network.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30071--30078, 2020.
\newblock \doi{10.1073/pnas.1907375117}.
\newblock URL \url{https://www.pnas.org/doi/abs/10.1073/pnas.1907375117}.

\bibitem[Belbute-Peres et~al.(2018)Belbute-Peres, Smith, Allen, Tenenbaum, and
  Kolter]{10.5555/3327757.3327820}
Filipe de~A. Belbute-Peres, Kevin~A. Smith, Kelsey~R. Allen, Joshua~B.
  Tenenbaum, and J.~Zico Kolter.
\newblock End-to-end differentiable physics for learning and control.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, NIPS'18, pp.\  7178–7189, Red Hook, NY,
  USA, 2018. Curran Associates Inc.

\bibitem[Blair~Jr(1957)]{Blair1957}
Clay Blair~Jr.
\newblock Passing of a great mind.
\newblock \emph{Life}, 25:\penalty0 96, 1957.

\bibitem[Chen et~al.(2022)Chen, Hong, Pajarinen, and
  Agrawal]{chen2022redeeming}
Eric Chen, Zhang-Wei Hong, Joni Pajarinen, and Pulkit Agrawal.
\newblock Redeeming intrinsic rewards via constrained optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 4996--5008, 2022.

\bibitem[Clark et~al.(2019)Clark, Khandelwal, Levy, and
  Manning]{clark-etal-2019-bert}
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher~D. Manning.
\newblock What does {BERT} look at? an analysis of {BERT}{'}s attention.
\newblock In \emph{Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing
  and Interpreting Neural Networks for NLP}, pp.\  276--286, Florence, Italy,
  August 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W19-4828}.
\newblock URL \url{https://aclanthology.org/W19-4828}.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformerxl}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V. Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context, 2019.

\bibitem[Frank(2023)]{frank2023bridging}
Michael~C Frank.
\newblock Bridging the data gap between children and large language models.
\newblock \emph{Trends in Cognitive Sciences}, 2023.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{frankle2019lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks, 2019.

\bibitem[Ge et~al.(2023)Ge, Luo, Kim, and Glass]{ge-etal-2023-entailment}
Jiaxin Ge, Hongyin Luo, Yoon Kim, and James Glass.
\newblock Entailment as robust self-learner.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  13803--13817,
  Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.772}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.772}.

\bibitem[Geva et~al.(2021)Geva, Schuster, Berant, and
  Levy]{geva-etal-2021-transformer}
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
\newblock Transformer feed-forward layers are key-value memories.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  5484--5495, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.446}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.446}.

\bibitem[Gopnik et~al.(1999)Gopnik, Meltzoff, and Kuhl]{gopnik1999scientist}
Alison Gopnik, Andrew~N Meltzoff, and Patricia~K Kuhl.
\newblock \emph{The scientist in the crib: Minds, brains, and how children
  learn.}
\newblock William Morrow \& Co, 1999.

\bibitem[Goyal \& Bengio(2020)Goyal and
  Bengio]{DBLP:journals/corr/abs-2011-15091}
Anirudh Goyal and Yoshua Bengio.
\newblock Inductive biases for deep learning of higher-level cognition.
\newblock \emph{CoRR}, abs/2011.15091, 2020.
\newblock URL \url{https://arxiv.org/abs/2011.15091}.

\bibitem[Goyal \& Bengio(2022)Goyal and Bengio]{goyal2022inductive}
Anirudh Goyal and Yoshua Bengio.
\newblock Inductive biases for deep learning of higher-level cognition.
\newblock \emph{Proceedings of the Royal Society A}, 478\penalty0
  (2266):\penalty0 20210068, 2022.

\bibitem[Gruber(1993)]{gruber1993translation}
Thomas~R Gruber.
\newblock A translation approach to portable ontology specifications.
\newblock \emph{Knowledge acquisition}, 5\penalty0 (2):\penalty0 199--220,
  1993.

\bibitem[Gruber(1995)]{gruber1995toward}
Thomas~R Gruber.
\newblock Toward principles for the design of ontologies used for knowledge
  sharing?
\newblock \emph{International journal of human-computer studies}, 43\penalty0
  (5-6):\penalty0 907--928, 1995.

\bibitem[Gruber(2013)]{gruber2013nature}
Thomas~R Gruber.
\newblock Nature, nurture, and knowledge acquisition.
\newblock \emph{International journal of human-computer studies}, 71\penalty0
  (2):\penalty0 191--194, 2013.

\bibitem[Henderson(2007)]{Henderson2007}
Harry Henderson.
\newblock \emph{Mathematics: Powerful Patterns Into Nature and Society}.
\newblock New York: Chelsea House, 2007.

\bibitem[Hernandez et~al.(2023)Hernandez, Li, and
  Andreas]{hernandez2023inspecting}
Evan Hernandez, Belinda~Z. Li, and Jacob Andreas.
\newblock Inspecting and editing knowledge representations in language models,
  2023.

\bibitem[Herrmann(1999)]{herrmann1999helen}
Dorothy Herrmann.
\newblock \emph{Helen Keller: a life}.
\newblock University of Chicago Press, 1999.

\bibitem[Hrbacek \& Jech(2017)Hrbacek and Jech]{hrbacek2017introduction}
Karel Hrbacek and Thomas Jech.
\newblock \emph{Introduction to set theory, revised and expanded}.
\newblock Crc Press, 2017.

\bibitem[Huth et~al.(2016)Huth, de~Heer, Griffiths, Theunissen, and
  Gallant]{DBLP:journals/nature/HuthHGTG16}
Alexander~G. Huth, Wendy~A. de~Heer, Thomas~L. Griffiths,
  Fr{\'{e}}d{\'{e}}ric~E. Theunissen, and Jack~L. Gallant.
\newblock Natural speech reveals the semantic maps that tile human cerebral
  cortex.
\newblock \emph{Nat.}, 532\penalty0 (7600):\penalty0 453--458, 2016.
\newblock \doi{10.1038/nature17637}.
\newblock URL \url{https://doi.org/10.1038/nature17637}.

\bibitem[Kingma \& Ba(2017)Kingma and Ba]{kingma2017adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2017.

\bibitem[Knight(2023)]{knight2023openai}
Will Knight.
\newblock Openai’s ceo says the age of giant ai models is already over.
\newblock \emph{Wired, April 17th}, 2023.

\bibitem[Lake et~al.(2017)Lake, Ullman, Tenenbaum, and
  Gershman]{lake2017building}
Brenden~M Lake, Tomer~D Ullman, Joshua~B Tenenbaum, and Samuel~J Gershman.
\newblock Building machines that learn and think like people.
\newblock \emph{Behavioral and brain sciences}, 40:\penalty0 e253, 2017.

\bibitem[Lamb et~al.(2021)Lamb, He, Goyal, Ke, Liao, Ravanelli, and
  Bengio]{DBLP:journals/corr/abs-2103-00336}
Alex Lamb, Di~He, Anirudh Goyal, Guolin Ke, Chien{-}Feng Liao, Mirco Ravanelli,
  and Yoshua Bengio.
\newblock Transformers with competitive ensembles of independent mechanisms.
\newblock \emph{CoRR}, abs/2103.00336, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.00336}.

\bibitem[Lerdahl \& Jackendoff(1983)Lerdahl and Jackendoff]{Lerdahl1983}
Fred Lerdahl and Ray Jackendoff.
\newblock \emph{A generative theory of tonal music}.
\newblock The MIT Press, Cambridge. MA, 1983.
\newblock ISBN 0262120941.

\bibitem[Li et~al.(2017)Li, Xu, Taylor, and
  Goldstein]{DBLP:journals/corr/abs-1712-09913}
Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{CoRR}, abs/1712.09913, 2017.
\newblock URL \url{http://arxiv.org/abs/1712.09913}.

\bibitem[Li et~al.(2023)Li, Karamolegkou, Kementchedjhieva, Abdou, Lehmann, and
  Søgaard]{li2023large}
Jiaang Li, Antonia Karamolegkou, Yova Kementchedjhieva, Mostafa Abdou, Sune
  Lehmann, and Anders Søgaard.
\newblock Large language models converge on brain-like word representations,
  2023.

\bibitem[Liu et~al.(2021)Liu, Dai, So, and Le]{liu2021pay}
Hanxiao Liu, Zihang Dai, David~R. So, and Quoc~V. Le.
\newblock Pay attention to mlps, 2021.

\bibitem[Lu \& Zhang(2022)Lu and Zhang]{lu2022unified}
Jieyu Lu and Yingkai Zhang.
\newblock Unified deep learning model for multitask reaction predictions with
  explanation.
\newblock \emph{Journal of Chemical Information and Modeling}, 62\penalty0
  (6):\penalty0 1376--1387, 2022.

\bibitem[MacKay(2003)]{mackay2003information}
David~JC MacKay.
\newblock \emph{Information theory, inference and learning algorithms}.
\newblock Cambridge university press, 2003.

\bibitem[Macrae(1992)]{Macrae1992}
Norman Macrae.
\newblock \emph{John von Neumann: The Scientific Genius Who Pioneered the
  Modern Computer, Game Theory, Nuclear Deterrence, and Much More.}
\newblock Pantheon Press, 1992.

\bibitem[Malik et~al.(2021)Malik, Wu, Vasavada, Song, Coots, Mitchell, Goodman,
  and Piech]{malik2021generative}
Ali Malik, Mike Wu, Vrinda Vasavada, Jinpeng Song, Madison Coots, John
  Mitchell, Noah Goodman, and Chris Piech.
\newblock Generative grading: Near human-level accuracy for automated feedback
  on richly structured problems, 2021.

\bibitem[Marcus et~al.(2023)Marcus, Leivada, and Murphy]{marcus2023sentence}
Gary Marcus, Evelina Leivada, and Elliot Murphy.
\newblock A sentence is worth a thousand pictures: Can large language models
  understand human language?, 2023.

\bibitem[Mazzaglia et~al.(2022)Mazzaglia, Catal, Verbelen, and
  Dhoedt]{mazzaglia2022curiosity}
Pietro Mazzaglia, Ozan Catal, Tim Verbelen, and Bart Dhoedt.
\newblock Curiosity-driven exploration via latent bayesian surprise.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pp.\  7752--7760, 2022.

\bibitem[Meng et~al.(2023)Meng, Sharma, Andonian, Belinkov, and
  Bau]{meng2023massediting}
Kevin Meng, Arnab~Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau.
\newblock Mass-editing memory in a transformer, 2023.

\bibitem[Merlin \& Toneva(2022)Merlin and Toneva]{merlin2022language}
Gabriele Merlin and Mariya Toneva.
\newblock Language models and brain alignment: beyond word-level semantics and
  prediction, 2022.

\bibitem[Mittal et~al.(2022)Mittal, Bengio, and Lajoie]{mittal2022modular}
Sarthak Mittal, Yoshua Bengio, and Guillaume Lajoie.
\newblock Is a modular architecture enough?, 2022.

\bibitem[Murphy(2002)]{murphy2002dynamic}
Kevin~P Murphy.
\newblock \emph{Dynamic bayesian networks: Representation, inference and
  learning}.
\newblock PhD thesis, University of California, Berkeley, 2002.

\bibitem[Neumann \& Redei(2005)Neumann and Redei]{Redei2005}
John~Von Neumann and Miklos Redei.
\newblock \emph{John von Neumann selected letters}.
\newblock American Mathematical Society, Providence, R.I.,, 2005.

\bibitem[Oh et~al.(2017)Oh, Singh, Lee, and Kohli]{oh2017zero}
Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli.
\newblock Zero-shot task generalization with multi-task deep reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2661--2670. PMLR, 2017.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Pearl(1988)]{pearl1988probabilistic}
Judea Pearl.
\newblock \emph{Probabilistic reasoning in intelligent systems: networks of
  plausible inference}.
\newblock Morgan kaufmann, 1988.

\bibitem[Pearl(1995)]{pearl1995causal}
Judea Pearl.
\newblock Causal diagrams for empirical research.
\newblock \emph{Biometrika}, 82\penalty0 (4):\penalty0 669--688, 1995.

\bibitem[Peterson et~al.(2019)Peterson, Verstynen, Yan, Calcini, Safavi, Ak,
  Kole, Zeldenrust, Celikel, Fan, et~al.]{peterson2019embracing}
Erik~J Peterson, Timothy~D Verstynen, Xuan Yan, Niccolo Calcini, Payam Safavi,
  Asli Ak, Koen Kole, Fleur Zeldenrust, Tansu Celikel, Yuanchan Fan, et~al.
\newblock Embracing curiosity eliminates the exploration-exploitation dilemma.
\newblock 2019.

\bibitem[Rogers et~al.(2020)Rogers, Kovaleva, and Rumshisky]{rogers2020primer}
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
\newblock A primer in bertology: What we know about how bert works, 2020.

\bibitem[Sartran et~al.(2022)Sartran, Barrett, Kuncoro, Stanojevi{\'c},
  Blunsom, and Dyer]{sartran2022transformer}
Laurent Sartran, Samuel Barrett, Adhiguna Kuncoro, Milo{\v{s}} Stanojevi{\'c},
  Phil Blunsom, and Chris Dyer.
\newblock Transformer grammars: Augmenting transformer language models with
  syntactic inductive biases at scale.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  10:\penalty0 1423--1439, 2022.

\bibitem[Schneider(2015)]{Schneider2015}
Gersting \&~Brinkman Schneider.
\newblock \emph{Invitation to Computer Science}.
\newblock Boston: Cengage Learning, 2015.

\bibitem[Shinn et~al.(2023)Shinn, Cassano, Labash, Gopinath, Narasimhan, and
  Yao]{shinn2023reflexion}
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan,
  and Shunyu Yao.
\newblock Reflexion: Language agents with verbal reinforcement learning, 2023.

\bibitem[Sun et~al.(2019)Sun, Myers, Vondrick, Murphy, and
  Schmid]{DBLP:journals/corr/abs-1904-01766}
Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid.
\newblock Videobert: {A} joint model for video and language representation
  learning.
\newblock \emph{CoRR}, abs/1904.01766, 2019.
\newblock URL \url{http://arxiv.org/abs/1904.01766}.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Bahri, and
  Metzler]{DBLP:journals/corr/abs-2009-06732}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: {A} survey.
\newblock \emph{CoRR}, abs/2009.06732, 2020.
\newblock URL \url{https://arxiv.org/abs/2009.06732}.

\bibitem[Thomaz et~al.(2006)Thomaz, Breazeal, et~al.]{thomaz2006reinforcement}
Andrea~Lockerd Thomaz, Cynthia Breazeal, et~al.
\newblock Reinforcement learning with human teachers: Evidence of feedback and
  guidance with implications for learning performance.
\newblock In \emph{Aaai}, volume~6, pp.\  1000--1005. Boston, MA, 2006.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need, 2017.

\bibitem[Veres(2022)]{veres2022large}
Csaba Veres.
\newblock Large language models are not models of natural language: they are
  corpus models, 2022.

\bibitem[Vigoda(2003)]{BenVigodaThesis}
Benjamin Vigoda.
\newblock Analog logic: Continuous-time analog circuits for statistical signal
  processing.
\newblock \emph{Online] Sep}, 2003.

\bibitem[Wang et~al.(2023)Wang, Zhang, Xie, Yao, Tian, Wang, Xi, Cheng, Liu,
  Zheng, and Chen]{wang2023easyedit}
Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun
  Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, and Huajun Chen.
\newblock Easyedit: An easy-to-use knowledge editing framework for large
  language models, 2023.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama,
  Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang, Dean, and
  Fedus]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H.
  Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
  Fedus.
\newblock Emergent abilities of large language models, 2022.

\bibitem[Welling(2019)]{welling2019}
Max Welling.
\newblock Do we still need models or just more data and compute.
\newblock \emph{University of Amsterdam, April}, 20, 2019.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Onta{\~{n}}{\'{o}}n, Pham, Ravula, Wang, Yang, and
  Ahmed]{DBLP:journals/corr/abs-2007-14062}
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti,
  Santiago Onta{\~{n}}{\'{o}}n, Philip Pham, Anirudh Ravula, Qifan Wang,
  Li~Yang, and Amr Ahmed.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{CoRR}, abs/2007.14062, 2020.
\newblock URL \url{https://arxiv.org/abs/2007.14062}.

\end{thebibliography}
