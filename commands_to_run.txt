First turn on the environment:
source /Users/benvigoda/src/inductive_transformer/venv/bin/activate
source ../inductive_transformer_position/venv/bin/activate

How we are currently running the system:
python main.py training_text.txt inference_text.txt --train --num_layer 2 --layer_width 2 --num_data_points 1000 --silence_google_sheet


To use the test weights (which are themselves set in the hyperparameters.py file) add the --weight_test flag and remove the --train flag
python main.py training_text.txt inference_text.txt --weight_test --num_layer 2 --layer_width 2 --num_data_points 1000 --silence_google_sheet

To run perturbation tests and only set some of the weights, add the --perturbation flag and keep the --train flag
python main.py training_text.txt inference_text.txt --perturbation --train --num_layer 2 --layer_width 2 --num_data_points 1000 --silence_google_sheet


From April 22, 2024
 main.py ../../training_text.txt ../../inference_text.txt --init_perturb_weights --train --num_layer 2 --layer_width 2 --num_data_points 1000 --silence_google_sheet
python main.py ../../training_text.txt ../../inference_text.txt --perturbation --train --num_layer 2 --layer_width 2 --num_data_points 1000 --silence_google_sheet

python
To get printing to google-sheet working you need to add a credentials.json file in the google_sheets_api directory.
The credentials.json should be in LastPass
The first time you try to print to google-sheet it will ask for some permissions and add a token.json file to the same directory
So long as this token.json file is there, you won't need to approve the permissions again.

From April 29, 2024
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py training_text.txt inference_text.txt --num_layer 2 --layer_width 2

May 24
python inductive_transformer/jax_transformer/train.py training_text.txt inference_text.txt --num_layer 2 --layer_width 2

June 14
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py training_text.txt --prompt_text inference_text.txt --perturb --num_layer 2 --layer_width 2

July 12
python inductive_transformer/jax_transformer/train.py 32_curated_sentences.txt --prompt_text inference_text_32.txt --num_layer 2 --layer_width 2


July 19
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 32_6_layer_sentences.txt --prompt_text inference_text.txt --num_layer 6 --layer_width 2

PYTHONPATH=. python inductive_transformer/jax_transformer/histogram_generations.py


July 23
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 32_2_layer_sentences.txt --prompt_text inference_text_more_synonyms.txt --num_layer 2 --layer_width 2


July 26
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 32_6_layer_sentences.txt --prompt_text inference_text_more_synonyms.txt --num_layer 6 --layer_width 2 --initialize_weights --lock_all_weights


August 2:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 30_6_layer_sentences.txt --prompt_text inference_text_6_words_sentence.txt --num_layer 6 --layer_width 2 --initialize_weights --lock_all_weights

Include number of samples to generate (per input prompt)
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 30_6_layer_sentences.txt --prompt_text inference_text_small.txt --num_layer 6 --layer_width 2 --initialize_weights --lock_all_weights --num_samples 50 --zero_out_weights_right_weights


mystery #1: SOLVE THIS FIRST
this command is only outputting green, yeah!  but new mystery it is not outputting a green for every blue, and the eloss is not 0.

mystery #2: SOLVE THIS NEXT IF IT DOESN'T BY SOLVING MYSTERY #1
same commit, same model, if we use only 2 layers, training no longer converges?!:
python inductive_transformer/jax_transformer/train.py 32_curated_sentences.txt --prompt_text inference_text_32.txt --num_layer 2 --layer_width 2

So for mystery #1, one question to ask is whether it thinks the weights we gave it are indeed a minima?  Let's unlock the weights and let it train:

PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 30_6_layer_sentences.txt --prompt_text inference_text_small.txt --num_layer 6 --layer_width 2 --initialize_weights --num_samples 10 --num_epochs 100 --zero_out_right_weights

This results in red all over the place, and no blue-greens.


Aug 3:

PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 30_6_layer_sentences_small.txt --prompt_text inference_text_big.txt --num_layer 6 --layer_width 2 --initialize_weights --num_samples 4 --num_epochs 0

--zero_out_right_weights



Aug 7:
To see what happens if we initialize to what we consider to be a minimum:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 60_6_layer_sentences_small_big.txt --prompt_text inference_text_big_small_balanced_synonyms.txt --num_layer 6 --layer_width 2 --num_samples 40 --num_epochs 300 --loss_threshold 0.018 --initialize_weights
seed = 1053121381
initial loss: 1.944e-02
final loss: 1.797e-02
still looks good!

With random initialization of weights:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 60_6_layer_sentences_small_big.txt --prompt_text inference_text_big_small_balanced_synonyms.txt --num_layer 6 --layer_width 2 --num_samples 4 --num_epochs 500 --loss_threshold 0.018
got all reds, and no bluegreens


Friday August 16th, 2024
With perturbation and some noise_value set:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 64_6_layer_sentences_balanced_dogs_birds.txt --prompt_text inference_text_small_wriggley.txt --num_layer 6 --layer_width 2 --num_samples 4 --num_epochs 50 --initialize_weights --perturb --noise_value 0.1 

And with all the synonyms included:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 48_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_dog_worm.txt --num_layer 6 --layer_width 2 --num_samples 400 --num_epochs 50 --initialize_weights --perturb --noise_value 0.001



Monday August 19

initialize to perfect with lots of synonyms in the training sentences and the inference prompt:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 48_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_dog_worm.txt --num_layer 6 --layer_width 2 --num_samples 400 --num_epochs 0 --initialize_weights
***this used to generate all green, but now doesn't so we broke something with how we initialize the weights


in all of these trainings, even if we do not initialize weights the position weights are initialized and cannot move:

first just verify that with 6 layers and two sentences (one on left, one on right), it runs and makes a histogram correctly:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 2_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_just_small_wriggley.txt --num_layer 6 --layer_width 2 --num_samples 4 --num_epochs 0 --initialize_weights
this generates only blue green

then see what happens when we train it for 50 epochs starting from perfect:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 2_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_just_small_wriggley.txt --num_layer 6 --layer_width 2 --num_samples 40 --initialize_weights --num_epochs 300 --perturb --noise_value 1.0
noise = 1.0, generates only blue green every time
noise = 10.0 causes nans.  why?

then see what happens when we train it for 100 epochs from scratch, which initializes weights between 0 and 1 which is pretty similar to --noise_value 1.0:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 2_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_just_small_wriggley.txt --num_layer 6 --layer_width 2 --num_samples 40 --num_epochs 300
doesn't converge as well as when we add noise.  we get reds, not just blue green.

Open questions even for two sentences:
questions: 1. why does it not converge from scratch?  2. why does noise =10.0 cause nans?
- also we were not adding noise to tokens.  we were only adding noise to attention.  Maybe that is why adding noise didn't hurt?
- Next: try adding noise to tokens and see if it converges to all blue green.
- If yes, then still ask questions 1 and 2.  If no, make new questions.
- Also is the loss for a bad situation worse than for a good one?

once we get it to fully converge a thing to try is, what if we do not train on entire data set?


then verify that with 6 layers and 4 sentences, it runs and makes a histogram correctly:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 4_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_just_small_wriggley.txt --num_layer 6 --layer_width 2 --num_samples 4 --num_epochs 0 --initialize_weights --num_epochs 0


then see what happens when we train it for 50 epochs starting from perfect:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 4_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_just_small_wriggley.txt --num_layer 6 --layer_width 2 --num_samples 400 --num_epochs 0 --initialize_weights --num_epochs 50


now perturb and it trains perfectly:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 4_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_just_small_wriggley.txt --num_layer 6 --layer_width 2 --num_samples 400 --num_epochs 0 --initialize_weights --num_epochs 50 --perturb --noise_value 0.001


now try training from scratch:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 4_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_just_small_wriggley.txt --num_layer 6 --layer_width 2 --num_samples 400 --num_epochs 100
it didn't land on blue-green

the perfect loss is loss: 5.208e-03
the trained loss is loss: 3.866e-03 which is actually LOWER! the histogram contain reds. and we are not hitting all the bluegreens - it is only generating the left-hand side (small).  the wriggley side is not good, but the small side is good.




now perturb and it trains perfectly:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 4_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_just_small_wriggley.txt --num_layer 6 --layer_width 2 --num_samples 400 --num_epochs 0 --initialize_weights --num_epochs 50 --perturb --noise_value 0.001


Monday August 21 - going back carefully through commands to make sure we understand the behavior completely


So with cats and dogs initialization to put dogs first on the left, and cats first on the right, we get bluegreens generated (trainings sentences, synonyms are dogs and cats)
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 60_6_layer_sentences_small_big.txt --prompt_text inference_text_big_small_balanced_synonyms.txt --num_layer 6 --layer_width 2 --num_samples 40 --num_epochs 0 --initialize_weights --catsanddogs

This works with all of the cat-worm synonyms available in the synonyms file:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 48_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_dog_worm.txt --num_layer 6 --layer_width 2 --num_samples 40 --num_epochs 0 --initialize_weights

Training on this gives us red:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 48_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_dog_worm.txt --num_layer 6 --layer_width 2 --num_samples 40 --num_epochs 100 --initialize_weights


Then we said let's go to 2 or 4 sentences 

Wednesday, February 5th, 2025:
Try this: 
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 48_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_dog_worm.txt --num_layer 6 --layer_width 2 --num_samples 40 --num_epochs 100 --initialize_weights


PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 4_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_just_small_wriggley.txt --num_layer 6 --layer_width 2 --num_samples 400 --num_epochs 100  --initialize_weights



Friday February 7th, 2025:

Interesting observation:
PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 4_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_just_small_wriggley.txt --num_layer 6 --layer_width 2 --num_samples 400 --num_epochs 40 --initialize_weights 
Yields NaNs


Versus


PYTHONPATH=. python inductive_transformer/jax_transformer/train.py 48_6_layer_sentences_balanced_dogs_birds_all_synonyms.txt --prompt_text inference_text_just_small_wriggley.txt --num_layer 6 --layer_width 2 --num_samples 400 --num_epochs 40 --initialize_weights   
Stays put with no NaNs
